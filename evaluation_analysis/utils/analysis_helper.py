#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Helper functions cho analysis agent evaluation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import ast
import json

# Thiáº¿t láº­p font cho tiáº¿ng Viá»‡t
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class AgentAnalyzer:
    def __init__(self, data_path="data_eval/results"):
        self.data_path = Path(data_path)
        self.agents_data = {}
        self.ground_truth_tools = {}
        
    def load_agent_data(self):
        """Load dá»¯ liá»‡u tá»« cÃ¡c file CSV káº¿t quáº£ evaluation"""
        agent_files = {
            'React': 'react_agent_eval_results.csv',
            'ReWOO': 'rewoo_agent_eval_results.csv', 
            'Reflexion': 'reflexion_agent_eval_results.csv',
            'Multi-Agent': 'multi_agent_eval_results.csv'
        }
        
        for agent_name, filename in agent_files.items():
            file_path = self.data_path / filename
            if file_path.exists():
                df = pd.read_csv(file_path)
                self.agents_data[agent_name] = df
                print(f"âœ… Loaded {len(df)} records for {agent_name}")
            else:
                print(f"âŒ File not found: {file_path}")
                
        return len(self.agents_data) > 0
    
    def load_ground_truth(self, synthetic_path="data_eval/synthetic_data/synthetic_news.csv"):
        """Load ground truth tools tá»« synthetic_news.csv"""
        synthetic_path = Path(synthetic_path)
        if not synthetic_path.exists():
            print(f"âŒ Ground truth file not found: {synthetic_path}")
            return False
            
        df_truth = pd.read_csv(synthetic_path)
        for _, row in df_truth.iterrows():
            query = row['query']
            tools_str = row['tools']
            try:
                tools_list = ast.literal_eval(tools_str)
                self.ground_truth_tools[query] = set(tools_list)
            except:
                # Fallback parsing
                tools_clean = tools_str.strip("[]'\"").replace("'", "").replace('"', '')
                tools_list = [t.strip() for t in tools_clean.split(',') if t.strip()]
                self.ground_truth_tools[query] = set(tools_list)
        
        print(f"âœ… Loaded ground truth for {len(self.ground_truth_tools)} queries")
        return True
    
    def parse_tools_used(self, tools_str):
        """Parse chuá»—i tools thÃ nh set"""
        if pd.isna(tools_str) or tools_str.strip() == '':
            return set()
        
        tools = set()
        for tool in str(tools_str).split(','):
            tool = tool.strip()
            if tool:
                tools.add(tool)
        return tools
    
    def get_required_tools(self, query):
        """Láº¥y tools cáº§n thiáº¿t tá»« ground truth"""
        if query in self.ground_truth_tools:
            return self.ground_truth_tools[query]
        
        # TÃ¬m similar query
        for gt_query, gt_tools in self.ground_truth_tools.items():
            if query.strip() == gt_query.strip():
                return gt_tools
        
        return set()
    
    def calculate_accuracy(self, df):
        """TÃ­nh accuracy - tá»‰ lá»‡ gá»i tools hoÃ n toÃ n Ä‘Ãºng dá»±a trÃªn ground truth"""
        correct_count = 0
        total_questions = len(df)
        
        for _, row in df.iterrows():
            required_tools = self.get_required_tools(row['input'])
            used_tools = self.parse_tools_used(row['tools'])
            
            # Loáº¡i bá» failed tools khá»i used_tools
            if row['failed_tools_count'] > 0 and not pd.isna(row['failed_tools']):
                failed_tools = self.parse_tools_used(row['failed_tools'])
                used_tools = used_tools - failed_tools
            
            # Kiá»ƒm tra xem cÃ³ gá»i Ä‘Ãºng hoÃ n toÃ n khÃ´ng
            if used_tools == required_tools:
                correct_count += 1
        
        return correct_count / total_questions if total_questions > 0 else 0
    
    def calculate_f1_metrics(self, df):
        """
        TÃ­nh F1, Precision, Recall dá»±a trÃªn ground truth tá»« synthetic_news.csv
        
        Precision = |Texp âˆ© Tact| / |Tact| - Tá»‰ lá»‡ tool Ä‘Æ°á»£c chá»n lÃ  cáº§n thiáº¿t
        Recall = |Texp âˆ© Tact| / |Texp| - Tá»‰ lá»‡ tool cáº§n thiáº¿t Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m tháº¥y
        F1 = 2 * (Precision * Recall) / (Precision + Recall)
        """
        tp = fp = fn = 0
        
        for _, row in df.iterrows():
            # Láº¥y tools cáº§n thiáº¿t tá»« ground truth (Texp)
            required_tools = self.get_required_tools(row['input'])
            # Láº¥y tools agent Ä‘Ã£ gá»i (Tact)
            used_tools = self.parse_tools_used(row['tools'])
            
            # Loáº¡i bá» failed tools khá»i Tact
            if row['failed_tools_count'] > 0 and not pd.isna(row['failed_tools']):
                failed_tools = self.parse_tools_used(row['failed_tools'])
                used_tools = used_tools - failed_tools
            
            # TÃ­nh TP, FP, FN
            tp += len(required_tools & used_tools)  # Tools Ä‘Ãºng (gá»i Ä‘Ãºng vÃ  cáº§n thiáº¿t)
            fp += len(used_tools - required_tools)  # Tools thá»«a (gá»i nhÆ°ng khÃ´ng cáº§n)
            fn += len(required_tools - used_tools)  # Tools thiáº¿u (cáº§n nhÆ°ng khÃ´ng gá»i)
        
        # TÃ­nh metrics
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return f1, precision, recall
    
    def calculate_tool_fail_rate(self, df):
        """TÃ­nh tá»‰ lá»‡ gá»i tool fail"""
        df_with_tools = df[df['tools'].notna() & (df['tools'].str.strip() != '')]
        if len(df_with_tools) == 0:
            return 0
        
        failed_calls = len(df_with_tools[df_with_tools['failed_tools_count'] > 0])
        return failed_calls / len(df_with_tools)
    
    def analyze_by_difficulty(self):
        """PhÃ¢n tÃ­ch metrics theo Ä‘á»™ khÃ³"""
        results = []
        
        for agent_name, df in self.agents_data.items():
            for difficulty in ['dá»…', 'khÃ³']:
                df_filtered = df[df['difficulty'] == difficulty]
                
                if len(df_filtered) > 0:
                    accuracy = self.calculate_accuracy(df_filtered)
                    f1, precision, recall = self.calculate_f1_metrics(df_filtered)
                    tool_fail_rate = self.calculate_tool_fail_rate(df_filtered)
                    
                    results.append({
                        'Agent': agent_name,
                        'Difficulty': difficulty,
                        'Accuracy': accuracy,
                        'F1_Score': f1,
                        'Precision': precision,
                        'Recall': recall,
                        'Tool_Fail_Rate': tool_fail_rate,
                        'Sample_Count': len(df_filtered)
                    })
        
        return pd.DataFrame(results)
    
    def analyze_failed_cases(self):
        """PhÃ¢n tÃ­ch cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i"""
        failed_cases = []
        
        for agent_name, df in self.agents_data.items():
            failed_df = df[df['failed_tools_count'] > 0]
            
            for _, row in failed_df.iterrows():
                failed_cases.append({
                    'Agent': agent_name,
                    'Query': row['input'][:100] + '...' if len(row['input']) > 100 else row['input'],
                    'Difficulty': row['difficulty'],
                    'Failed_Tools': row['failed_tools'],
                    'Failed_Count': row['failed_tools_count'],
                    'All_Tools': row['tools']
                })
        
        return pd.DataFrame(failed_cases)

def create_folder_structure(base_path="results"):
    """Táº¡o cáº¥u trÃºc thÆ° má»¥c"""
    base_path = Path(base_path)
    
    folders = [
        "metrics",
        "visualizations", 
        "rankings",
        "detailed_reports",
        "raw_data"
    ]
    
    for folder in folders:
        folder_path = base_path / folder
        folder_path.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ Created folder: {folder_path}")

def save_metrics_separately(results_df, base_path="results/metrics"):
    """LÆ°u tá»«ng metric vÃ o file riÃªng"""
    base_path = Path(base_path)
    
    # 1. Accuracy results
    accuracy_df = results_df[['Agent', 'Difficulty', 'Accuracy', 'Sample_Count']].copy()
    accuracy_df.to_csv(base_path / "accuracy_results.csv", index=False)
    
    # 2. F1 Score results
    f1_df = results_df[['Agent', 'Difficulty', 'F1_Score', 'Precision', 'Recall', 'Sample_Count']].copy()
    f1_df.to_csv(base_path / "f1_score_results.csv", index=False)
    
    # 3. Tool performance
    tool_df = results_df[['Agent', 'Difficulty', 'Tool_Fail_Rate', 'Sample_Count']].copy()
    tool_df['Tool_Success_Rate'] = 1 - tool_df['Tool_Fail_Rate']
    tool_df.to_csv(base_path / "tool_performance_results.csv", index=False)
    
    # 4. Summary metrics
    results_df.to_csv(base_path / "summary_metrics.csv", index=False)
    
    print(f"ğŸ’¾ Saved metrics to {base_path}")

def create_individual_rankings(results_df, base_path="results/rankings"):
    """Táº¡o file ranking riÃªng cho tá»«ng metric"""
    base_path = Path(base_path)
    
    summary = results_df.groupby('Agent').agg({
        'Accuracy': 'mean',
        'F1_Score': 'mean',
        'Precision': 'mean',
        'Recall': 'mean', 
        'Tool_Fail_Rate': 'mean'
    }).round(4)
    
    # Accuracy ranking
    with open(base_path / "accuracy_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ¯ Xáº¾P Háº NG ACCURACY (Cao nháº¥t â†’ Tháº¥p nháº¥t)\n")
        f.write("="*50 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Accuracy', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Accuracy']:.4f} ({row['Accuracy']*100:.2f}%)\n")
    
    # F1 Score ranking  
    with open(base_path / "f1_score_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ¯ Xáº¾P Háº NG F1 SCORE (Cao nháº¥t â†’ Tháº¥p nháº¥t)\n")
        f.write("="*50 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('F1_Score', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['F1_Score']:.4f}\n")
            f.write(f"   - Precision: {row['Precision']:.4f}\n")
            f.write(f"   - Recall: {row['Recall']:.4f}\n\n")
    
    # Precision ranking
    with open(base_path / "precision_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ¯ Xáº¾P Háº NG PRECISION (Cao nháº¥t â†’ Tháº¥p nháº¥t)\n")
        f.write("="*50 + "\n")
        f.write("ğŸ“Š Precision = |Texp âˆ© Tact| / |Tact|\n")
        f.write("ğŸ’¡ Tá»‰ lá»‡ tool Ä‘Æ°á»£c chá»n lÃ  cáº§n thiáº¿t (Ã­t gá»i thá»«a)\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Precision', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Precision']:.4f} ({row['Precision']*100:.2f}%)\n")
            if row['Precision'] < 0.7:
                f.write("   âš ï¸  ThÆ°á»ng gá»i tools thá»«a\n")
            elif row['Precision'] > 0.9:
                f.write("   âœ… Ráº¥t Ã­t gá»i tools thá»«a\n")
            f.write("\n")
    
    # Recall ranking
    with open(base_path / "recall_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ¯ Xáº¾P Háº NG RECALL (Cao nháº¥t â†’ Tháº¥p nháº¥t)\n")
        f.write("="*50 + "\n")
        f.write("ğŸ“Š Recall = |Texp âˆ© Tact| / |Texp|\n")
        f.write("ğŸ’¡ Tá»‰ lá»‡ tool cáº§n thiáº¿t Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m tháº¥y (Ã­t bá» sÃ³t)\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Recall', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Recall']:.4f} ({row['Recall']*100:.2f}%)\n")
            if row['Recall'] < 0.7:
                f.write("   âš ï¸  ThÆ°á»ng bá» sÃ³t tools cáº§n thiáº¿t\n")
            elif row['Recall'] > 0.9:
                f.write("   âœ… Ráº¥t Ã­t bá» sÃ³t tools\n")
            f.write("\n")
    
    # Tool performance ranking
    with open(base_path / "tool_performance_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ¯ Xáº¾P Háº NG TOOL PERFORMANCE (Tháº¥p fail rate â†’ Cao fail rate)\n")
        f.write("="*60 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Tool_Fail_Rate', ascending=True).iterrows(), 1):
            success_rate = 1 - row['Tool_Fail_Rate']
            f.write(f"{i}. {agent}:\n")
            f.write(f"   - Success Rate: {success_rate:.4f} ({success_rate*100:.2f}%)\n")
            f.write(f"   - Fail Rate: {row['Tool_Fail_Rate']:.4f} ({row['Tool_Fail_Rate']*100:.2f}%)\n\n")
    
    # Overall ranking (tá»•ng há»£p)
    with open(base_path / "overall_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ† Xáº¾P Háº NG Tá»”NG THá»‚\n")
        f.write("="*30 + "\n\n")
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng há»£p (normalized)
        normalized = summary.copy()
        normalized['Accuracy_norm'] = normalized['Accuracy'] / normalized['Accuracy'].max()
        normalized['F1_norm'] = normalized['F1_Score'] / normalized['F1_Score'].max()
        normalized['Tool_norm'] = (1 - normalized['Tool_Fail_Rate']) / (1 - normalized['Tool_Fail_Rate']).max()
        normalized['Overall_Score'] = (normalized['Accuracy_norm'] + normalized['F1_norm'] + normalized['Tool_norm']) / 3
        
        for i, (agent, row) in enumerate(normalized.sort_values('Overall_Score', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Overall_Score']:.4f}\n")
            f.write(f"   - Accuracy: {summary.loc[agent, 'Accuracy']:.4f}\n")
            f.write(f"   - F1 Score: {summary.loc[agent, 'F1_Score']:.4f}\n")
            f.write(f"   - Tool Success: {1-summary.loc[agent, 'Tool_Fail_Rate']:.4f}\n\n")
    
    print(f"ğŸ“Š Created rankings in {base_path}")

def save_detailed_reports(results_df, failed_cases_df, base_path="results/detailed_reports"):
    """Táº¡o cÃ¡c bÃ¡o cÃ¡o chi tiáº¿t"""
    base_path = Path(base_path)
    
    summary = results_df.groupby('Agent').agg({
        'Accuracy': 'mean',
        'F1_Score': 'mean',
        'Precision': 'mean',
        'Recall': 'mean',
        'Tool_Fail_Rate': 'mean',
        'Sample_Count': 'sum'
    }).round(4)
    
    # Executive Summary
    with open(base_path / "executive_summary.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ“‹ TÃ“M Táº®T ÄIá»€U HÃ€NH - ÄÃNH GIÃ HIá»†U SUáº¤T AGENT\n")
        f.write("="*60 + "\n\n")
        
        best_accuracy = summary['Accuracy'].idxmax()
        best_f1 = summary['F1_Score'].idxmax() 
        best_tool = summary['Tool_Fail_Rate'].idxmin()
        
        f.write("ğŸ† Káº¾T QUáº¢ CHÃNH:\n")
        f.write(f"â€¢ Agent tá»‘t nháº¥t vá» Accuracy: {best_accuracy} ({summary.loc[best_accuracy, 'Accuracy']:.3f})\n")
        f.write(f"â€¢ Agent tá»‘t nháº¥t vá» F1 Score: {best_f1} ({summary.loc[best_f1, 'F1_Score']:.3f})\n")
        f.write(f"â€¢ Agent tin cáº­y nháº¥t (Ã­t lá»—i): {best_tool} ({(1-summary.loc[best_tool, 'Tool_Fail_Rate']):.3f})\n\n")
        
        f.write("ğŸ“Š THá»NG KÃŠ Tá»”NG QUAN:\n")
        f.write(f"â€¢ Tá»•ng sá»‘ cÃ¢u há»i Ä‘Ã¡nh giÃ¡: {results_df['Sample_Count'].sum()//2} (má»—i agent)\n")
        f.write(f"â€¢ Sá»‘ agent Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡: {len(summary)}\n")
        f.write(f"â€¢ Accuracy trung bÃ¬nh: {summary['Accuracy'].mean():.3f}\n")
        f.write(f"â€¢ F1 Score trung bÃ¬nh: {summary['F1_Score'].mean():.3f}\n")
        f.write(f"â€¢ Tool Success Rate trung bÃ¬nh: {(1-summary['Tool_Fail_Rate']).mean():.3f}\n\n")
        
        f.write("ğŸ’¡ KHUYáº¾N NGHá»Š:\n")
        if best_accuracy == best_f1 == best_tool:
            f.write(f"â€¢ {best_accuracy} lÃ  lá»±a chá»n tá»‘t nháº¥t cho táº¥t cáº£ metrics\n")
        else:
            f.write(f"â€¢ Äá»ƒ accuracy cao: Chá»n {best_accuracy}\n")
            f.write(f"â€¢ Äá»ƒ cÃ¢n báº±ng precision/recall: Chá»n {best_f1}\n") 
            f.write(f"â€¢ Äá»ƒ Ã­t lá»—i nháº¥t: Chá»n {best_tool}\n")
    
    print(f"ğŸ“„ Created detailed reports in {base_path}")

def save_failed_cases_analysis(failed_cases_df, base_path="results/raw_data"):
    """LÆ°u phÃ¢n tÃ­ch cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i"""
    base_path = Path(base_path)
    failed_cases_df.to_csv(base_path / "failed_cases_analysis.csv", index=False)
    print(f"âŒ Saved failed cases analysis to {base_path}/failed_cases_analysis.csv") 