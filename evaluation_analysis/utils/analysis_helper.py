#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Helper functions cho analysis agent evaluation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import ast
import json

# Thi·∫øt l·∫≠p font cho ti·∫øng Vi·ªát
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class AgentAnalyzer:
    def __init__(self, data_path="data_eval/results"):
        self.data_path = Path(data_path)
        self.agents_data = {}
        self.ground_truth_tools = {}
        
    def load_agent_data(self):
        """Load d·ªØ li·ªáu t·ª´ c√°c file CSV k·∫øt qu·∫£ evaluation"""
        agent_files = {
            'React': 'react_agent_eval_results.csv',
            'ReWOO': 'rewoo_agent_eval_results.csv', 
            'Reflexion': 'reflexion_agent_eval_results.csv',
            'Multi-Agent': 'multi_agent_eval_results.csv'
        }
        
        for agent_name, filename in agent_files.items():
            file_path = self.data_path / filename
            if file_path.exists():
                df = pd.read_csv(file_path)
                self.agents_data[agent_name] = df
                print(f"‚úÖ Loaded {len(df)} records for {agent_name}")
            else:
                print(f"‚ùå File not found: {file_path}")
                
        return len(self.agents_data) > 0
    
    def load_ground_truth(self, synthetic_path="data_eval/synthetic_data/synthetic_news.csv"):
        """Load ground truth tools t·ª´ synthetic_news.csv"""
        synthetic_path = Path(synthetic_path)
        if not synthetic_path.exists():
            print(f"‚ùå Ground truth file not found: {synthetic_path}")
            return False
            
        df_truth = pd.read_csv(synthetic_path)
        for _, row in df_truth.iterrows():
            query = row['query']
            tools_str = row['tools']
            try:
                tools_list = ast.literal_eval(tools_str)
                self.ground_truth_tools[query] = set(tools_list)
            except:
                # Fallback parsing
                tools_clean = tools_str.strip("[]'\"").replace("'", "").replace('"', '')
                tools_list = [t.strip() for t in tools_clean.split(',') if t.strip()]
                self.ground_truth_tools[query] = set(tools_list)
        
        print(f"‚úÖ Loaded ground truth for {len(self.ground_truth_tools)} queries")
        return True
    
    def parse_tools_used(self, tools_str):
        """Parse chu·ªói tools th√†nh set"""
        if pd.isna(tools_str) or tools_str.strip() == '':
            return set()
        
        tools = set()
        for tool in str(tools_str).split(','):
            tool = tool.strip()
            if tool:
                tools.add(tool)
        return tools
    
    def get_required_tools(self, query):
        """L·∫•y tools c·∫ßn thi·∫øt t·ª´ ground truth"""
        if query in self.ground_truth_tools:
            return self.ground_truth_tools[query]
        
        # T√¨m similar query
        for gt_query, gt_tools in self.ground_truth_tools.items():
            if query.strip() == gt_query.strip():
                return gt_tools
        
        return set()
    
    def calculate_accuracy(self, df):
        """T√≠nh accuracy - t·ªâ l·ªá g·ªçi tools ho√†n to√†n ƒë√∫ng"""
        total_questions = len(df)
        correct_tool_calls = len(df[df['failed_tools_count'] == 0])
        return correct_tool_calls / total_questions if total_questions > 0 else 0
    
    def calculate_f1_metrics(self, df):
        """T√≠nh F1, Precision, Recall d·ª±a tr√™n ground truth"""
        tp = fp = fn = 0
        
        for _, row in df.iterrows():
            required_tools = self.get_required_tools(row['input'])
            used_tools = self.parse_tools_used(row['tools'])
            
            # Lo·∫°i b·ªè failed tools
            if row['failed_tools_count'] > 0 and not pd.isna(row['failed_tools']):
                failed_tools = self.parse_tools_used(row['failed_tools'])
                used_tools = used_tools - failed_tools
            
            tp += len(required_tools & used_tools)  # Tools ƒë√∫ng
            fp += len(used_tools - required_tools)  # Tools th·ª´a
            fn += len(required_tools - used_tools)  # Tools thi·∫øu
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return f1, precision, recall
    
    def calculate_tool_fail_rate(self, df):
        """T√≠nh t·ªâ l·ªá g·ªçi tool fail"""
        df_with_tools = df[df['tools'].notna() & (df['tools'].str.strip() != '')]
        if len(df_with_tools) == 0:
            return 0
        
        failed_calls = len(df_with_tools[df_with_tools['failed_tools_count'] > 0])
        return failed_calls / len(df_with_tools)
    
    def analyze_by_difficulty(self):
        """Ph√¢n t√≠ch metrics theo ƒë·ªô kh√≥"""
        results = []
        
        for agent_name, df in self.agents_data.items():
            for difficulty in ['d·ªÖ', 'kh√≥']:
                df_filtered = df[df['difficulty'] == difficulty]
                
                if len(df_filtered) > 0:
                    accuracy = self.calculate_accuracy(df_filtered)
                    f1, precision, recall = self.calculate_f1_metrics(df_filtered)
                    tool_fail_rate = self.calculate_tool_fail_rate(df_filtered)
                    
                    results.append({
                        'Agent': agent_name,
                        'Difficulty': difficulty,
                        'Accuracy': accuracy,
                        'F1_Score': f1,
                        'Precision': precision,
                        'Recall': recall,
                        'Tool_Fail_Rate': tool_fail_rate,
                        'Sample_Count': len(df_filtered)
                    })
        
        return pd.DataFrame(results)
    
    def analyze_failed_cases(self):
        """Ph√¢n t√≠ch c√°c tr∆∞·ªùng h·ª£p th·∫•t b·∫°i"""
        failed_cases = []
        
        for agent_name, df in self.agents_data.items():
            failed_df = df[df['failed_tools_count'] > 0]
            
            for _, row in failed_df.iterrows():
                failed_cases.append({
                    'Agent': agent_name,
                    'Query': row['input'][:100] + '...' if len(row['input']) > 100 else row['input'],
                    'Difficulty': row['difficulty'],
                    'Failed_Tools': row['failed_tools'],
                    'Failed_Count': row['failed_tools_count'],
                    'All_Tools': row['tools']
                })
        
        return pd.DataFrame(failed_cases)

def create_folder_structure(base_path="evaluation_analysis/results"):
    """T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c"""
    base_path = Path(base_path)
    
    folders = [
        "metrics",
        "visualizations", 
        "rankings",
        "detailed_reports",
        "raw_data"
    ]
    
    for folder in folders:
        folder_path = base_path / folder
        folder_path.mkdir(parents=True, exist_ok=True)
        print(f"üìÅ Created folder: {folder_path}")

def save_metrics_separately(results_df, base_path="evaluation_analysis/results/metrics"):
    """L∆∞u t·ª´ng metric v√†o file ri√™ng"""
    base_path = Path(base_path)
    
    # 1. Accuracy results
    accuracy_df = results_df[['Agent', 'Difficulty', 'Accuracy', 'Sample_Count']].copy()
    accuracy_df.to_csv(base_path / "accuracy_results.csv", index=False)
    
    # 2. F1 Score results
    f1_df = results_df[['Agent', 'Difficulty', 'F1_Score', 'Precision', 'Recall', 'Sample_Count']].copy()
    f1_df.to_csv(base_path / "f1_score_results.csv", index=False)
    
    # 3. Tool performance
    tool_df = results_df[['Agent', 'Difficulty', 'Tool_Fail_Rate', 'Sample_Count']].copy()
    tool_df['Tool_Success_Rate'] = 1 - tool_df['Tool_Fail_Rate']
    tool_df.to_csv(base_path / "tool_performance_results.csv", index=False)
    
    # 4. Summary metrics
    results_df.to_csv(base_path / "summary_metrics.csv", index=False)
    
    print(f"üíæ Saved metrics to {base_path}")

def create_individual_rankings(results_df, base_path="evaluation_analysis/results/rankings"):
    """T·∫°o file ranking ri√™ng cho t·ª´ng metric"""
    base_path = Path(base_path)
    
    summary = results_df.groupby('Agent').agg({
        'Accuracy': 'mean',
        'F1_Score': 'mean',
        'Precision': 'mean',
        'Recall': 'mean', 
        'Tool_Fail_Rate': 'mean'
    }).round(4)
    
    # Accuracy ranking
    with open(base_path / "accuracy_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("üéØ X·∫æP H·∫†NG ACCURACY (Cao nh·∫•t ‚Üí Th·∫•p nh·∫•t)\n")
        f.write("="*50 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Accuracy', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Accuracy']:.4f} ({row['Accuracy']*100:.2f}%)\n")
    
    # F1 Score ranking  
    with open(base_path / "f1_score_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("üéØ X·∫æP H·∫†NG F1 SCORE (Cao nh·∫•t ‚Üí Th·∫•p nh·∫•t)\n")
        f.write("="*50 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('F1_Score', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['F1_Score']:.4f}\n")
            f.write(f"   - Precision: {row['Precision']:.4f}\n")
            f.write(f"   - Recall: {row['Recall']:.4f}\n\n")
    
    # Tool performance ranking
    with open(base_path / "tool_performance_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("üéØ X·∫æP H·∫†NG TOOL PERFORMANCE (Th·∫•p fail rate ‚Üí Cao fail rate)\n")
        f.write("="*60 + "\n\n")
        for i, (agent, row) in enumerate(summary.sort_values('Tool_Fail_Rate', ascending=True).iterrows(), 1):
            success_rate = 1 - row['Tool_Fail_Rate']
            f.write(f"{i}. {agent}:\n")
            f.write(f"   - Success Rate: {success_rate:.4f} ({success_rate*100:.2f}%)\n")
            f.write(f"   - Fail Rate: {row['Tool_Fail_Rate']:.4f} ({row['Tool_Fail_Rate']*100:.2f}%)\n\n")
    
    # Overall ranking (t·ªïng h·ª£p)
    with open(base_path / "overall_ranking.txt", 'w', encoding='utf-8') as f:
        f.write("üèÜ X·∫æP H·∫†NG T·ªîNG TH·ªÇ\n")
        f.write("="*30 + "\n\n")
        
        # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p (normalized)
        normalized = summary.copy()
        normalized['Accuracy_norm'] = normalized['Accuracy'] / normalized['Accuracy'].max()
        normalized['F1_norm'] = normalized['F1_Score'] / normalized['F1_Score'].max()
        normalized['Tool_norm'] = (1 - normalized['Tool_Fail_Rate']) / (1 - normalized['Tool_Fail_Rate']).max()
        normalized['Overall_Score'] = (normalized['Accuracy_norm'] + normalized['F1_norm'] + normalized['Tool_norm']) / 3
        
        for i, (agent, row) in enumerate(normalized.sort_values('Overall_Score', ascending=False).iterrows(), 1):
            f.write(f"{i}. {agent}: {row['Overall_Score']:.4f}\n")
            f.write(f"   - Accuracy: {summary.loc[agent, 'Accuracy']:.4f}\n")
            f.write(f"   - F1 Score: {summary.loc[agent, 'F1_Score']:.4f}\n")
            f.write(f"   - Tool Success: {1-summary.loc[agent, 'Tool_Fail_Rate']:.4f}\n\n")
    
    print(f"üìä Created rankings in {base_path}")

def save_detailed_reports(results_df, failed_cases_df, base_path="evaluation_analysis/results/detailed_reports"):
    """T·∫°o c√°c b√°o c√°o chi ti·∫øt"""
    base_path = Path(base_path)
    
    summary = results_df.groupby('Agent').agg({
        'Accuracy': 'mean',
        'F1_Score': 'mean',
        'Precision': 'mean',
        'Recall': 'mean',
        'Tool_Fail_Rate': 'mean',
        'Sample_Count': 'sum'
    }).round(4)
    
    # Executive Summary
    with open(base_path / "executive_summary.txt", 'w', encoding='utf-8') as f:
        f.write("üìã T√ìM T·∫ÆT ƒêI·ªÄU H√ÄNH - ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T AGENT\n")
        f.write("="*60 + "\n\n")
        
        best_accuracy = summary['Accuracy'].idxmax()
        best_f1 = summary['F1_Score'].idxmax() 
        best_tool = summary['Tool_Fail_Rate'].idxmin()
        
        f.write("üèÜ K·∫æT QU·∫¢ CH√çNH:\n")
        f.write(f"‚Ä¢ Agent t·ªët nh·∫•t v·ªÅ Accuracy: {best_accuracy} ({summary.loc[best_accuracy, 'Accuracy']:.3f})\n")
        f.write(f"‚Ä¢ Agent t·ªët nh·∫•t v·ªÅ F1 Score: {best_f1} ({summary.loc[best_f1, 'F1_Score']:.3f})\n")
        f.write(f"‚Ä¢ Agent tin c·∫≠y nh·∫•t (√≠t l·ªói): {best_tool} ({(1-summary.loc[best_tool, 'Tool_Fail_Rate']):.3f})\n\n")
        
        f.write("üìä TH·ªêNG K√ä T·ªîNG QUAN:\n")
        f.write(f"‚Ä¢ T·ªïng s·ªë c√¢u h·ªèi ƒë√°nh gi√°: {results_df['Sample_Count'].sum()//2} (m·ªói agent)\n")
        f.write(f"‚Ä¢ S·ªë agent ƒë∆∞·ª£c ƒë√°nh gi√°: {len(summary)}\n")
        f.write(f"‚Ä¢ Accuracy trung b√¨nh: {summary['Accuracy'].mean():.3f}\n")
        f.write(f"‚Ä¢ F1 Score trung b√¨nh: {summary['F1_Score'].mean():.3f}\n")
        f.write(f"‚Ä¢ Tool Success Rate trung b√¨nh: {(1-summary['Tool_Fail_Rate']).mean():.3f}\n\n")
        
        f.write("üí° KHUY·∫æN NGH·ªä:\n")
        if best_accuracy == best_f1 == best_tool:
            f.write(f"‚Ä¢ {best_accuracy} l√† l·ª±a ch·ªçn t·ªët nh·∫•t cho t·∫•t c·∫£ metrics\n")
        else:
            f.write(f"‚Ä¢ ƒê·ªÉ accuracy cao: Ch·ªçn {best_accuracy}\n")
            f.write(f"‚Ä¢ ƒê·ªÉ c√¢n b·∫±ng precision/recall: Ch·ªçn {best_f1}\n") 
            f.write(f"‚Ä¢ ƒê·ªÉ √≠t l·ªói nh·∫•t: Ch·ªçn {best_tool}\n")
    
    print(f"üìÑ Created detailed reports in {base_path}")

def save_failed_cases_analysis(failed_cases_df, base_path="evaluation_analysis/results/raw_data"):
    """L∆∞u ph√¢n t√≠ch c√°c tr∆∞·ªùng h·ª£p th·∫•t b·∫°i"""
    base_path = Path(base_path)
    failed_cases_df.to_csv(base_path / "failed_cases_analysis.csv", index=False)
    print(f"‚ùå Saved failed cases analysis to {base_path}/failed_cases_analysis.csv") 