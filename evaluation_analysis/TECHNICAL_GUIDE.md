# ğŸ”§ TECHNICAL GUIDE - HÆ°á»›ng dáº«n ká»¹ thuáº­t chi tiáº¿t

## ğŸ¯ Má»¥c Ä‘Ã­ch

Guide nÃ y dÃ nh cho developers muá»‘n:
- ğŸ” **Hiá»ƒu cÃ¡ch script hoáº¡t Ä‘á»™ng**
- âš™ï¸ **Customize phÃ¢n tÃ­ch**
- ğŸ› **Debug vÃ  troubleshoot**
- ğŸ“Š **ThÃªm metrics má»›i**
- ğŸ”§ **Modify visualization**

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### **Core Components:**

```
run_comparison_analysis.py     # Entry point chÃ­nh
â”œâ”€â”€ AgentAnalyzer             # Class chÃ­nh xá»­ lÃ½ data
â”œâ”€â”€ create_visualization_charts()   # Táº¡o táº¥t cáº£ biá»ƒu Ä‘á»“
â”œâ”€â”€ create_technical_details_report()  # BÃ¡o cÃ¡o ká»¹ thuáº­t
â””â”€â”€ main()                    # Orchestrator chÃ­nh

utils/analysis_helper.py       # Helper functions
â”œâ”€â”€ AgentAnalyzer class       # Data processing & metrics
â”œâ”€â”€ create_folder_structure() # Setup directories
â”œâ”€â”€ save_metrics_separately() # Export CSV files
â”œâ”€â”€ create_individual_rankings() # Generate rankings
â””â”€â”€ save_detailed_reports()   # Create reports
```

### **Data Flow:**

```
../evaluation/data_eval/results/*.csv
    â†“ [AgentAnalyzer.load_agent_data()]
Agent Data (4 DataFrames)
    â†“ [load_ground_truth()]
+ Ground Truth (synthetic_news.csv)
    â†“ [analyze_by_difficulty()]
Results DataFrame
    â†“ [Parallel Processing]
â”œâ”€â”€ save_metrics_separately() â†’ results/metrics/*.csv
â”œâ”€â”€ create_visualization_charts() â†’ results/visualizations/*.png
â”œâ”€â”€ create_individual_rankings() â†’ results/rankings/*.txt
â””â”€â”€ save_detailed_reports() â†’ results/detailed_reports/*.txt
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng Script

### **1. Cháº¡y analysis Ä‘áº§y Ä‘á»§:**
```bash
cd evaluation_analysis
python run_comparison_analysis.py
```

### **2. Chá»‰ táº¡o charts:**
```bash
python run_comparison_analysis.py --charts-only
```

### **3. Chá»‰ tÃ­nh metrics:**
```bash
python run_comparison_analysis.py --metrics-only
```

## ğŸ“Š Metrics Implementation Details

### **1. Accuracy (NEW - Ground Truth Based)**
```python
def calculate_accuracy(self, df):
    """Perfect match vá»›i ground truth"""
    correct_count = 0
    for _, row in df.iterrows():
        required_tools = self.get_required_tools(row['input'])  # Texp
        used_tools = self.parse_tools_used(row['tools'])        # Tact
        
        # Remove failed tools
        if row['failed_tools_count'] > 0:
            failed_tools = self.parse_tools_used(row['failed_tools'])
            used_tools = used_tools - failed_tools
        
        if used_tools == required_tools:  # Perfect match
            correct_count += 1
    
    return correct_count / len(df)
```

### **2. Precision & Recall**
```python
def calculate_f1_metrics(self, df):
    """Dá»±a trÃªn ground truth tá»« synthetic_news.csv"""
    tp = fp = fn = 0
    
    for _, row in df.iterrows():
        required_tools = self.get_required_tools(row['input'])  # Texp
        used_tools = self.parse_tools_used(row['tools'])        # Tact (no failed)
        
        tp += len(required_tools & used_tools)  # Correct tools
        fp += len(used_tools - required_tools)  # Extra tools  
        fn += len(required_tools - used_tools)  # Missing tools
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return f1, precision, recall
```

### **3. Tool Fail Rate (Unchanged)**
```python
def calculate_tool_fail_rate(self, df):
    """Tá»‰ lá»‡ execution failures"""
    df_with_tools = df[df['tools'].notna() & (df['tools'].str.strip() != '')]
    if len(df_with_tools) == 0:
        return 0
    
    failed_calls = len(df_with_tools[df_with_tools['failed_tools_count'] > 0])
    return failed_calls / len(df_with_tools)
```

## ğŸ”§ Customization Guide

### **1. ThÃªm metric má»›i:**

**BÆ°á»›c 1:** ThÃªm function trong `AgentAnalyzer`:
```python
def calculate_your_metric(self, df):
    """Your custom metric"""
    # Your logic here
    return metric_value
```

**BÆ°á»›c 2:** Cáº­p nháº­t `analyze_by_difficulty()`:
```python
your_metric = self.calculate_your_metric(df_filtered)

results.append({
    'Agent': agent_name,
    'Difficulty': difficulty,
    'Your_Metric': your_metric,  # Add this line
    # ... existing fields
})
```

**BÆ°á»›c 3:** Cáº­p nháº­t visualization vÃ  reports theo nhu cáº§u.

### **2. ThÃªm visualization má»›i:**

**Trong `create_visualization_charts()`:**
```python
# Your new chart
fig, ax = plt.subplots(figsize=(12, 8))
# Your plotting logic
plt.savefig(base_path / "your_chart.png", dpi=300, bbox_inches='tight')
plt.close()
```

### **3. Customize file paths:**

**Trong `run_comparison_analysis.py`:**
```python
# Change data paths
analyzer = AgentAnalyzer("your/custom/path")
analyzer.load_ground_truth("your/ground_truth.csv")

# Change output paths
create_visualization_charts(results_df, "your/output/path")
```

## ğŸ“ File Structure Details

### **Input Files (Required):**
```
../evaluation/data_eval/results/
â”œâ”€â”€ react_agent_eval_results.csv      # React agent results
â”œâ”€â”€ rewoo_agent_eval_results.csv      # ReWOO agent results  
â”œâ”€â”€ reflexion_agent_eval_results.csv  # Reflexion agent results
â””â”€â”€ multi_agent_eval_results.csv      # Multi-agent results

../evaluation/data_eval/synthetic_data/
â””â”€â”€ synthetic_news.csv                # Ground truth labels
```

### **Expected CSV Format:**
```csv
input,difficulty,output,tools,failed_tools,failed_tools_count
"Query text","dá»…|khÃ³","Response","tool1,tool2","failed_tool",1
```

### **Output Structure:**
```
results/
â”œâ”€â”€ metrics/                    # Structured data
â”‚   â”œâ”€â”€ accuracy_results.csv   # Accuracy by agent/difficulty
â”‚   â”œâ”€â”€ f1_score_results.csv   # P/R/F1 by agent/difficulty
â”‚   â”œâ”€â”€ tool_performance_results.csv  # Tool metrics
â”‚   â””â”€â”€ summary_metrics.csv    # All metrics combined
â”œâ”€â”€ visualizations/             # PNG charts
â”‚   â”œâ”€â”€ accuracy_comparison.png
â”‚   â”œâ”€â”€ f1_score_comparison.png
â”‚   â”œâ”€â”€ precision_recall_analysis.png  # NEW
â”‚   â”œâ”€â”€ difficulty_analysis.png
â”‚   â”œâ”€â”€ tool_metrics_heatmap.png
â”‚   â””â”€â”€ overall_dashboard.png
â”œâ”€â”€ rankings/                   # Text rankings
â”‚   â”œâ”€â”€ accuracy_ranking.txt
â”‚   â”œâ”€â”€ f1_score_ranking.txt
â”‚   â”œâ”€â”€ precision_ranking.txt   # NEW
â”‚   â”œâ”€â”€ recall_ranking.txt      # NEW
â”‚   â”œâ”€â”€ tool_performance_ranking.txt
â”‚   â””â”€â”€ overall_ranking.txt
â”œâ”€â”€ detailed_reports/           # Analysis reports
â”‚   â”œâ”€â”€ executive_summary.txt
â”‚   â”œâ”€â”€ technical_details.txt
â”‚   â””â”€â”€ full_analysis_report.txt
â””â”€â”€ raw_data/                   # Complete datasets
    â”œâ”€â”€ complete_results.csv
    â””â”€â”€ failed_cases_analysis.csv
```

## ğŸ› Debugging & Troubleshooting

### **Common Issues:**

#### **1. Data Loading Errors:**
```python
# Debug data loading
analyzer = AgentAnalyzer("../evaluation/data_eval/results")
print(f"Found {len(analyzer.agents_data)} agents")
for agent, df in analyzer.agents_data.items():
    print(f"{agent}: {len(df)} records")
```

#### **2. Ground Truth Mismatch:**
```python
# Check ground truth loading
analyzer.load_ground_truth("../evaluation/data_eval/synthetic_data/synthetic_news.csv")
print(f"Ground truth queries: {len(analyzer.ground_truth_tools)}")

# Check query matching
for query in df['input'].head(5):
    required = analyzer.get_required_tools(query)
    print(f"Query: {query[:50]}...")
    print(f"Required tools: {required}")
```

#### **3. Visualization Errors:**
```python
# Debug matplotlib
import matplotlib
print(f"Backend: {matplotlib.get_backend()}")

# Force non-interactive backend
matplotlib.use('Agg')
```

#### **4. Performance Issues:**
```python
# Profile slow operations
import time

start = time.time()
results_df = analyzer.analyze_by_difficulty()
print(f"Analysis took {time.time() - start:.2f} seconds")
```

### **Validation Checks:**

```python
# Validate results
def validate_results(results_df):
    # Check completeness
    expected_agents = ['React', 'ReWOO', 'Reflexion', 'Multi-Agent']
    expected_difficulties = ['dá»…', 'khÃ³']
    
    for agent in expected_agents:
        for diff in expected_difficulties:
            subset = results_df[(results_df['Agent'] == agent) & 
                              (results_df['Difficulty'] == diff)]
            assert len(subset) == 1, f"Missing {agent} - {diff}"
    
    # Check metric ranges
    assert results_df['Accuracy'].between(0, 1).all()
    assert results_df['Precision'].between(0, 1).all()
    assert results_df['Recall'].between(0, 1).all()
    assert results_df['F1_Score'].between(0, 1).all()
    
    print("âœ… All validation checks passed!")
```

## ğŸ”„ Development Workflow

### **1. Making Changes:**
```bash
# 1. Edit code
vim utils/analysis_helper.py

# 2. Test changes
python run_comparison_analysis.py --metrics-only

# 3. Full test
python run_comparison_analysis.py

# 4. Validate outputs
ls -la results/
```

### **2. Adding New Features:**
1. **Plan the feature** - Define inputs/outputs
2. **Implement core logic** - Add to `AgentAnalyzer`
3. **Update main script** - Integrate into workflow
4. **Add visualization** - If needed
5. **Test thoroughly** - Validate with real data
6. **Document changes** - Update this guide

### **3. Performance Optimization:**
```python
# Use pandas vectorization
df['metric'] = df.apply(lambda row: calculate_metric(row), axis=1)

# Better: Vectorized operations
df['metric'] = vectorized_calculate_metric(df)

# Memory efficient processing
for chunk in pd.read_csv('large_file.csv', chunksize=1000):
    process_chunk(chunk)
```

## ğŸ“š Dependencies

### **Required Packages:**
```python
pandas>=1.5.0          # Data processing
matplotlib>=3.6.0       # Plotting
seaborn>=0.12.0         # Statistical visualization  
numpy>=1.24.0           # Numerical operations
pathlib                 # Path handling (built-in)
argparse                # CLI arguments (built-in)
ast                     # String parsing (built-in)
```

### **Installation:**
```bash
pip install pandas matplotlib seaborn numpy
```

## ğŸ¯ Best Practices

1. **Always validate data before processing**
2. **Use relative paths for portability**
3. **Add error handling for edge cases**
4. **Document new metrics clearly**
5. **Test with different data sizes**
6. **Keep backup of original data**
7. **Use descriptive variable names**
8. **Add type hints for clarity**

---

*ğŸ”§ Äá»ƒ thÃªm features má»›i hoáº·c fix bugs, follow workflow nÃ y vÃ  update documentation tÆ°Æ¡ng á»©ng.* 