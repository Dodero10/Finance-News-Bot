#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script chÃ­nh Ä‘á»ƒ cháº¡y analysis so sÃ¡nh cÃ¡c agent vÃ  lÆ°u káº¿t quáº£ cÃ³ tá»• chá»©c
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import seaborn as sns
import numpy as np
from datetime import datetime

# Import helper functions
from utils.analysis_helper import (
    AgentAnalyzer, 
    create_folder_structure,
    save_metrics_separately,
    create_individual_rankings,
    save_detailed_reports,
    save_failed_cases_analysis
)

def create_visualization_charts(results_df, base_path="evaluation_analysis/results/visualizations"):
    """Táº¡o táº¥t cáº£ biá»ƒu Ä‘á»“ vÃ  lÆ°u riÃªng biá»‡t"""
    base_path = Path(base_path)
    
    # Style setup
    plt.style.use('seaborn-v0_8')
    colors = {'React': '#FF6B6B', 'ReWOO': '#4ECDC4', 'Reflexion': '#45B7D1', 'Multi-Agent': '#96CEB4'}
    
    # 1. Accuracy Comparison
    fig, ax = plt.subplots(figsize=(12, 8))
    accuracy_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='Accuracy')
    accuracy_pivot.plot(kind='bar', ax=ax, color=['#FF9999', '#FF6666'], width=0.8)
    ax.set_title('So sÃ¡nh Accuracy theo Agent vÃ  Äá»™ khÃ³', fontsize=16, fontweight='bold', pad=20)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('Agent', fontsize=12)
    ax.legend(title='Äá»™ khÃ³', title_fontsize=12, fontsize=11)
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "accuracy_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. F1 Score Comparison
    fig, ax = plt.subplots(figsize=(12, 8))
    f1_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='F1_Score')
    f1_pivot.plot(kind='bar', ax=ax, color=['#99CCFF', '#6699FF'], width=0.8)
    ax.set_title('So sÃ¡nh F1 Score theo Agent vÃ  Äá»™ khÃ³', fontsize=16, fontweight='bold', pad=20)
    ax.set_ylabel('F1 Score', fontsize=12)
    ax.set_xlabel('Agent', fontsize=12)
    ax.legend(title='Äá»™ khÃ³', title_fontsize=12, fontsize=11)
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', alpha=0.3)
    
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "f1_score_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Tool Performance Heatmap
    fig, ax = plt.subplots(figsize=(12, 8))
    summary = results_df.groupby('Agent')[['Accuracy', 'F1_Score', 'Precision', 'Recall']].mean()
    summary['Tool_Success_Rate'] = 1 - results_df.groupby('Agent')['Tool_Fail_Rate'].mean()
    
    sns.heatmap(summary.T, annot=True, fmt='.3f', cmap='RdYlGn', 
                ax=ax, cbar_kws={'label': 'Score'}, square=True)
    ax.set_title('Heatmap Hiá»‡u suáº¥t Tool theo Agent', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('Agent', fontsize=12)
    ax.set_ylabel('Metrics', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(base_path / "tool_metrics_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. Difficulty Analysis
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # Accuracy by difficulty
    easy_data = results_df[results_df['Difficulty'] == 'dá»…']
    hard_data = results_df[results_df['Difficulty'] == 'khÃ³']
    
    x = np.arange(len(easy_data))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, easy_data['Accuracy'], width, label='Dá»…', color='lightgreen', alpha=0.8)
    bars2 = ax1.bar(x + width/2, hard_data['Accuracy'], width, label='KhÃ³', color='lightcoral', alpha=0.8)
    
    ax1.set_xlabel('Agent', fontsize=12)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_title('Accuracy theo Äá»™ khÃ³', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(easy_data['Agent'], rotation=45)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.annotate(f'{height:.3f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=9)
    
    # F1 Score by difficulty
    bars3 = ax2.bar(x - width/2, easy_data['F1_Score'], width, label='Dá»…', color='lightblue', alpha=0.8)
    bars4 = ax2.bar(x + width/2, hard_data['F1_Score'], width, label='KhÃ³', color='orange', alpha=0.8)
    
    ax2.set_xlabel('Agent', fontsize=12)
    ax2.set_ylabel('F1 Score', fontsize=12) 
    ax2.set_title('F1 Score theo Äá»™ khÃ³', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(easy_data['Agent'], rotation=45)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    for bars in [bars3, bars4]:
        for bar in bars:
            height = bar.get_height()
            ax2.annotate(f'{height:.3f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "difficulty_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. Overall Dashboard (4-panel view)
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Dashboard Tá»•ng quan - So sÃ¡nh hiá»‡u suáº¥t cÃ¡c Agent', fontsize=18, fontweight='bold')
    
    # Panel 1: Accuracy
    ax = axes[0, 0]
    accuracy_pivot.plot(kind='bar', ax=ax, color=['#FF9999', '#FF6666'])
    ax.set_title('Accuracy', fontweight='bold')
    ax.set_ylabel('Score')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='Äá»™ khÃ³')
    
    # Panel 2: F1 Score
    ax = axes[0, 1]
    f1_pivot.plot(kind='bar', ax=ax, color=['#99CCFF', '#6699FF'])
    ax.set_title('F1 Score', fontweight='bold')
    ax.set_ylabel('Score')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='Äá»™ khÃ³')
    
    # Panel 3: Tool Fail Rate
    ax = axes[1, 0]
    fail_rate_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='Tool_Fail_Rate')
    fail_rate_pivot.plot(kind='bar', ax=ax, color=['#FFB366', '#FF8C42'])
    ax.set_title('Tool Fail Rate (Tháº¥p hÆ¡n = Tá»‘t hÆ¡n)', fontweight='bold')
    ax.set_ylabel('Fail Rate')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='Äá»™ khÃ³')
    
    # Panel 4: Overall Heatmap
    ax = axes[1, 1]
    overview_data = results_df.groupby('Agent')[['Accuracy', 'F1_Score']].mean()
    overview_data['Tool_Success'] = 1 - results_df.groupby('Agent')['Tool_Fail_Rate'].mean()
    sns.heatmap(overview_data.T, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax)
    ax.set_title('Tá»•ng quan (Cao hÆ¡n = Tá»‘t hÆ¡n)', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(base_path / "overall_dashboard.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š Created all visualizations in {base_path}")

def create_technical_details_report(results_df, analyzer, base_path="evaluation_analysis/results/detailed_reports"):
    """Táº¡o bÃ¡o cÃ¡o chi tiáº¿t ká»¹ thuáº­t"""
    base_path = Path(base_path)
    
    with open(base_path / "technical_details.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ”§ BÃO CÃO CHI TIáº¾T Ká»¸ THUáº¬T\n")
        f.write("="*50 + "\n\n")
        
        f.write("ğŸ“Š PHÆ¯Æ NG PHÃP TÃNH TOÃN:\n")
        f.write("-" * 30 + "\n")
        f.write("1. ACCURACY:\n")
        f.write("   - CÃ´ng thá»©c: (Sá»‘ cÃ¢u cÃ³ failed_tools_count = 0) / Tá»•ng sá»‘ cÃ¢u\n")
        f.write("   - Ã nghÄ©a: Tá»‰ lá»‡ agent gá»i tools hoÃ n toÃ n Ä‘Ãºng, khÃ´ng cÃ³ lá»—i\n\n")
        
        f.write("2. F1 SCORE:\n")
        f.write("   - Dá»±a trÃªn ground truth tá»« synthetic_news.csv\n")
        f.write("   - TP: Tools Ä‘Æ°á»£c gá»i Ä‘Ãºng vÃ  cáº§n thiáº¿t\n")
        f.write("   - FP: Tools Ä‘Æ°á»£c gá»i nhÆ°ng khÃ´ng cáº§n thiáº¿t (thá»«a)\n")
        f.write("   - FN: Tools cáº§n thiáº¿t nhÆ°ng khÃ´ng Ä‘Æ°á»£c gá»i (thiáº¿u)\n")
        f.write("   - Precision = TP / (TP + FP)\n")
        f.write("   - Recall = TP / (TP + FN)\n")
        f.write("   - F1 = 2 * (Precision * Recall) / (Precision + Recall)\n\n")
        
        f.write("3. TOOL FAIL RATE:\n")
        f.write("   - CÃ´ng thá»©c: (Sá»‘ cÃ¢u cÃ³ failed_tools_count > 0) / Tá»•ng sá»‘ cÃ¢u cÃ³ gá»i tools\n")
        f.write("   - Ã nghÄ©a: Tá»‰ lá»‡ lá»—i khi agent thá»±c thi tools\n\n")
        
        f.write("ğŸ“ˆ THá»NG KÃŠ CHI TIáº¾T:\n")
        f.write("-" * 30 + "\n")
        
        # Chi tiáº¿t theo agent
        for agent in results_df['Agent'].unique():
            agent_data = results_df[results_df['Agent'] == agent]
            f.write(f"\nğŸ¤– {agent}:\n")
            
            for difficulty in ['dá»…', 'khÃ³']:
                diff_data = agent_data[agent_data['Difficulty'] == difficulty]
                if len(diff_data) > 0:
                    row = diff_data.iloc[0]
                    f.write(f"   ğŸ“ CÃ¢u {difficulty}:\n")
                    f.write(f"      - Sá»‘ máº«u: {row['Sample_Count']}\n")
                    f.write(f"      - Accuracy: {row['Accuracy']:.4f} ({row['Accuracy']*100:.2f}%)\n")
                    f.write(f"      - F1 Score: {row['F1_Score']:.4f}\n")
                    f.write(f"      - Precision: {row['Precision']:.4f}\n")
                    f.write(f"      - Recall: {row['Recall']:.4f}\n")
                    f.write(f"      - Tool Fail Rate: {row['Tool_Fail_Rate']:.4f} ({row['Tool_Fail_Rate']*100:.2f}%)\n")
        
        f.write(f"\nğŸ“‹ GROUND TRUTH INFORMATION:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Sá»‘ lÆ°á»£ng ground truth queries: {len(analyzer.ground_truth_tools)}\n")
        f.write("Tools Ä‘Æ°á»£c sá»­ dá»¥ng trong ground truth:\n")
        all_tools = set()
        for tools in analyzer.ground_truth_tools.values():
            all_tools.update(tools)
        for tool in sorted(all_tools):
            f.write(f"   - {tool}\n")

def create_full_analysis_report(results_df, base_path="evaluation_analysis/results/detailed_reports"):
    """Táº¡o bÃ¡o cÃ¡o phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§"""
    base_path = Path(base_path)
    
    with open(base_path / "full_analysis_report.txt", 'w', encoding='utf-8') as f:
        f.write("ğŸ“Š BÃO CÃO PHÃ‚N TÃCH Äáº¦Y Äá»¦ - SO SÃNH HIá»†U SUáº¤T CÃC AGENT\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"ğŸ“… Thá»i gian táº¡o bÃ¡o cÃ¡o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("ğŸ“‹ Tá»”NG QUAN Dá»® LIá»†U:\n")
        f.write("-" * 40 + "\n")
        total_samples = results_df['Sample_Count'].sum()
        agents_count = len(results_df['Agent'].unique())
        f.write(f"â€¢ Tá»•ng sá»‘ máº«u Ä‘Ã¡nh giÃ¡: {total_samples}\n")
        f.write(f"â€¢ Sá»‘ agent Ä‘Æ°á»£c so sÃ¡nh: {agents_count}\n")
        f.write(f"â€¢ Sá»‘ Ä‘á»™ khÃ³: {len(results_df['Difficulty'].unique())}\n")
        f.write(f"â€¢ Metrics Ä‘Ã¡nh giÃ¡: Accuracy, F1 Score, Precision, Recall, Tool Fail Rate\n\n")
        
        f.write("ğŸ“Š Báº¢NG Káº¾T QUáº¢ CHI TIáº¾T:\n")
        f.write("-" * 40 + "\n")
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.4f}'.format)
        f.write(results_df.to_string(index=False))
        f.write("\n\n")
        
        f.write("ğŸ“ˆ PHÃ‚N TÃCH THEO AGENT:\n")
        f.write("-" * 40 + "\n")
        summary = results_df.groupby('Agent').agg({
            'Accuracy': ['mean', 'std'],
            'F1_Score': ['mean', 'std'],
            'Tool_Fail_Rate': ['mean', 'std']
        }).round(4)
        
        for agent in summary.index:
            f.write(f"\nğŸ¤– {agent}:\n")
            f.write(f"   â€¢ Accuracy: {summary.loc[agent, ('Accuracy', 'mean')]:.4f} Â± {summary.loc[agent, ('Accuracy', 'std')]:.4f}\n")
            f.write(f"   â€¢ F1 Score: {summary.loc[agent, ('F1_Score', 'mean')]:.4f} Â± {summary.loc[agent, ('F1_Score', 'std')]:.4f}\n")
            f.write(f"   â€¢ Tool Fail Rate: {summary.loc[agent, ('Tool_Fail_Rate', 'mean')]:.4f} Â± {summary.loc[agent, ('Tool_Fail_Rate', 'std')]:.4f}\n")
        
        f.write("\nğŸ“Š PHÃ‚N TÃCH THEO Äá»˜ KHÃ“:\n")
        f.write("-" * 40 + "\n")
        
        for difficulty in ['dá»…', 'khÃ³']:
            diff_data = results_df[results_df['Difficulty'] == difficulty]
            f.write(f"\nğŸ“ CÃ¢u {difficulty}:\n")
            f.write(f"   â€¢ Accuracy trung bÃ¬nh: {diff_data['Accuracy'].mean():.4f}\n")
            f.write(f"   â€¢ F1 Score trung bÃ¬nh: {diff_data['F1_Score'].mean():.4f}\n")
            f.write(f"   â€¢ Tool Fail Rate trung bÃ¬nh: {diff_data['Tool_Fail_Rate'].mean():.4f}\n")
            f.write(f"   â€¢ Agent tá»‘t nháº¥t (Accuracy): {diff_data.loc[diff_data['Accuracy'].idxmax(), 'Agent']}\n")
            f.write(f"   â€¢ Agent tá»‘t nháº¥t (F1): {diff_data.loc[diff_data['F1_Score'].idxmax(), 'Agent']}\n")

def main():
    parser = argparse.ArgumentParser(description='Cháº¡y analysis so sÃ¡nh cÃ¡c agent')
    parser.add_argument('--charts-only', action='store_true', help='Chá»‰ táº¡o biá»ƒu Ä‘á»“')
    parser.add_argument('--metrics-only', action='store_true', help='Chá»‰ tÃ­nh metrics')
    args = parser.parse_args()
    
    print("ğŸš€ Báº®T Äáº¦U ANALYSIS SO SÃNH CÃC AGENT")
    print("="*50)
    
    # Táº¡o cáº¥u trÃºc thÆ° má»¥c
    print("\nğŸ“ Táº¡o cáº¥u trÃºc thÆ° má»¥c...")
    create_folder_structure()
    
    # Initialize analyzer
    analyzer = AgentAnalyzer("data_eval/results")
    
    # Load data
    print("\nğŸ“¥ Loading dá»¯ liá»‡u...")
    if not analyzer.load_agent_data():
        print("âŒ KhÃ´ng thá»ƒ load dá»¯ liá»‡u agent!")
        return
    
    if not analyzer.load_ground_truth("data_eval/synthetic_data/synthetic_news.csv"):
        print("âŒ KhÃ´ng thá»ƒ load ground truth!")
        return
    
    # Analyze by difficulty
    print("\nğŸ” PhÃ¢n tÃ­ch theo Ä‘á»™ khÃ³...")
    results_df = analyzer.analyze_by_difficulty()
    failed_cases_df = analyzer.analyze_failed_cases()
    
    if not args.charts_only:
        # Save metrics separately
        print("\nğŸ’¾ LÆ°u metrics...")
        save_metrics_separately(results_df)
        
        # Create rankings
        print("\nğŸ† Táº¡o rankings...")
        create_individual_rankings(results_df)
        
        # Save raw data
        print("\nğŸ’¿ LÆ°u raw data...")
        results_df.to_csv("evaluation_analysis/results/raw_data/complete_results.csv", index=False)
        save_failed_cases_analysis(failed_cases_df)
        
        # Create detailed reports
        print("\nğŸ“„ Táº¡o bÃ¡o cÃ¡o chi tiáº¿t...")
        save_detailed_reports(results_df, failed_cases_df)
        create_technical_details_report(results_df, analyzer)
        create_full_analysis_report(results_df)
    
    if not args.metrics_only:
        # Create visualizations
        print("\nğŸ“Š Táº¡o biá»ƒu Ä‘á»“...")
        create_visualization_charts(results_df)
    
    print(f"\nâœ… HOÃ€N THÃ€NH! Táº¥t cáº£ káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c evaluation_analysis/results/")
    print("\nğŸ“‹ CÃ¡ch xem káº¿t quáº£:")
    print("   â€¢ Xem ranking nhanh: evaluation_analysis/results/rankings/overall_ranking.txt")
    print("   â€¢ Xem bÃ¡o cÃ¡o tÃ³m táº¯t: evaluation_analysis/results/detailed_reports/executive_summary.txt")
    print("   â€¢ Xem biá»ƒu Ä‘á»“: evaluation_analysis/results/visualizations/")
    print("   â€¢ Dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch thÃªm: evaluation_analysis/results/raw_data/complete_results.csv")
    
    return results_df

if __name__ == "__main__":
    results = main() 