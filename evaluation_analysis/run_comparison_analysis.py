#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script ch√≠nh ƒë·ªÉ ch·∫°y analysis so s√°nh c√°c agent v√† l∆∞u k·∫øt qu·∫£ c√≥ t·ªï ch·ª©c
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import seaborn as sns
import numpy as np
from datetime import datetime

# Import helper functions
from utils.analysis_helper import (
    AgentAnalyzer, 
    create_folder_structure,
    save_metrics_separately,
    create_individual_rankings,
    save_detailed_reports,
    save_failed_cases_analysis
)

def create_visualization_charts(results_df, base_path="evaluation_analysis/results/visualizations"):
    """T·∫°o t·∫•t c·∫£ bi·ªÉu ƒë·ªì v√† l∆∞u ri√™ng bi·ªát"""
    base_path = Path(base_path)
    
    # Style setup
    plt.style.use('seaborn-v0_8')
    colors = {'React': '#FF6B6B', 'ReWOO': '#4ECDC4', 'Reflexion': '#45B7D1', 'Multi-Agent': '#96CEB4'}
    
    # 1. Accuracy Comparison
    fig, ax = plt.subplots(figsize=(12, 8))
    accuracy_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='Accuracy')
    accuracy_pivot.plot(kind='bar', ax=ax, color=['#FF9999', '#FF6666'], width=0.8)
    ax.set_title('So s√°nh Accuracy theo Agent v√† ƒê·ªô kh√≥', fontsize=16, fontweight='bold', pad=20)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('Agent', fontsize=12)
    ax.legend(title='ƒê·ªô kh√≥', title_fontsize=12, fontsize=11)
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "accuracy_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. F1 Score Comparison
    fig, ax = plt.subplots(figsize=(12, 8))
    f1_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='F1_Score')
    f1_pivot.plot(kind='bar', ax=ax, color=['#99CCFF', '#6699FF'], width=0.8)
    ax.set_title('So s√°nh F1 Score theo Agent v√† ƒê·ªô kh√≥', fontsize=16, fontweight='bold', pad=20)
    ax.set_ylabel('F1 Score', fontsize=12)
    ax.set_xlabel('Agent', fontsize=12)
    ax.legend(title='ƒê·ªô kh√≥', title_fontsize=12, fontsize=11)
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', alpha=0.3)
    
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "f1_score_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Tool Performance Heatmap
    fig, ax = plt.subplots(figsize=(12, 8))
    summary = results_df.groupby('Agent')[['Accuracy', 'F1_Score', 'Precision', 'Recall']].mean()
    summary['Tool_Success_Rate'] = 1 - results_df.groupby('Agent')['Tool_Fail_Rate'].mean()
    
    sns.heatmap(summary.T, annot=True, fmt='.3f', cmap='RdYlGn', 
                ax=ax, cbar_kws={'label': 'Score'}, square=True)
    ax.set_title('Heatmap Hi·ªáu su·∫•t Tool theo Agent', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('Agent', fontsize=12)
    ax.set_ylabel('Metrics', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(base_path / "tool_metrics_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. Difficulty Analysis
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # Accuracy by difficulty
    easy_data = results_df[results_df['Difficulty'] == 'd·ªÖ']
    hard_data = results_df[results_df['Difficulty'] == 'kh√≥']
    
    x = np.arange(len(easy_data))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, easy_data['Accuracy'], width, label='D·ªÖ', color='lightgreen', alpha=0.8)
    bars2 = ax1.bar(x + width/2, hard_data['Accuracy'], width, label='Kh√≥', color='lightcoral', alpha=0.8)
    
    ax1.set_xlabel('Agent', fontsize=12)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_title('Accuracy theo ƒê·ªô kh√≥', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(easy_data['Agent'], rotation=45)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.annotate(f'{height:.3f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=9)
    
    # F1 Score by difficulty
    bars3 = ax2.bar(x - width/2, easy_data['F1_Score'], width, label='D·ªÖ', color='lightblue', alpha=0.8)
    bars4 = ax2.bar(x + width/2, hard_data['F1_Score'], width, label='Kh√≥', color='orange', alpha=0.8)
    
    ax2.set_xlabel('Agent', fontsize=12)
    ax2.set_ylabel('F1 Score', fontsize=12) 
    ax2.set_title('F1 Score theo ƒê·ªô kh√≥', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(easy_data['Agent'], rotation=45)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    for bars in [bars3, bars4]:
        for bar in bars:
            height = bar.get_height()
            ax2.annotate(f'{height:.3f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(base_path / "difficulty_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. Overall Dashboard (4-panel view)
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Dashboard T·ªïng quan - So s√°nh hi·ªáu su·∫•t c√°c Agent', fontsize=18, fontweight='bold')
    
    # Panel 1: Accuracy
    ax = axes[0, 0]
    accuracy_pivot.plot(kind='bar', ax=ax, color=['#FF9999', '#FF6666'])
    ax.set_title('Accuracy', fontweight='bold')
    ax.set_ylabel('Score')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='ƒê·ªô kh√≥')
    
    # Panel 2: F1 Score
    ax = axes[0, 1]
    f1_pivot.plot(kind='bar', ax=ax, color=['#99CCFF', '#6699FF'])
    ax.set_title('F1 Score', fontweight='bold')
    ax.set_ylabel('Score')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='ƒê·ªô kh√≥')
    
    # Panel 3: Tool Fail Rate
    ax = axes[1, 0]
    fail_rate_pivot = results_df.pivot(index='Agent', columns='Difficulty', values='Tool_Fail_Rate')
    fail_rate_pivot.plot(kind='bar', ax=ax, color=['#FFB366', '#FF8C42'])
    ax.set_title('Tool Fail Rate (Th·∫•p h∆°n = T·ªët h∆°n)', fontweight='bold')
    ax.set_ylabel('Fail Rate')
    ax.tick_params(axis='x', rotation=45)
    ax.legend(title='ƒê·ªô kh√≥')
    
    # Panel 4: Overall Heatmap
    ax = axes[1, 1]
    overview_data = results_df.groupby('Agent')[['Accuracy', 'F1_Score']].mean()
    overview_data['Tool_Success'] = 1 - results_df.groupby('Agent')['Tool_Fail_Rate'].mean()
    sns.heatmap(overview_data.T, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax)
    ax.set_title('T·ªïng quan (Cao h∆°n = T·ªët h∆°n)', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(base_path / "overall_dashboard.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Created all visualizations in {base_path}")

def create_technical_details_report(results_df, analyzer, base_path="evaluation_analysis/results/detailed_reports"):
    """T·∫°o b√°o c√°o chi ti·∫øt k·ªπ thu·∫≠t"""
    base_path = Path(base_path)
    
    with open(base_path / "technical_details.txt", 'w', encoding='utf-8') as f:
        f.write("üîß B√ÅO C√ÅO CHI TI·∫æT K·ª∏ THU·∫¨T\n")
        f.write("="*50 + "\n\n")
        
        f.write("üìä PH∆Ø∆†NG PH√ÅP T√çNH TO√ÅN:\n")
        f.write("-" * 30 + "\n")
        f.write("1. ACCURACY:\n")
        f.write("   - C√¥ng th·ª©c: (S·ªë c√¢u c√≥ failed_tools_count = 0) / T·ªïng s·ªë c√¢u\n")
        f.write("   - √ù nghƒ©a: T·ªâ l·ªá agent g·ªçi tools ho√†n to√†n ƒë√∫ng, kh√¥ng c√≥ l·ªói\n\n")
        
        f.write("2. F1 SCORE:\n")
        f.write("   - D·ª±a tr√™n ground truth t·ª´ synthetic_news.csv\n")
        f.write("   - TP: Tools ƒë∆∞·ª£c g·ªçi ƒë√∫ng v√† c·∫ßn thi·∫øt\n")
        f.write("   - FP: Tools ƒë∆∞·ª£c g·ªçi nh∆∞ng kh√¥ng c·∫ßn thi·∫øt (th·ª´a)\n")
        f.write("   - FN: Tools c·∫ßn thi·∫øt nh∆∞ng kh√¥ng ƒë∆∞·ª£c g·ªçi (thi·∫øu)\n")
        f.write("   - Precision = TP / (TP + FP)\n")
        f.write("   - Recall = TP / (TP + FN)\n")
        f.write("   - F1 = 2 * (Precision * Recall) / (Precision + Recall)\n\n")
        
        f.write("3. TOOL FAIL RATE:\n")
        f.write("   - C√¥ng th·ª©c: (S·ªë c√¢u c√≥ failed_tools_count > 0) / T·ªïng s·ªë c√¢u c√≥ g·ªçi tools\n")
        f.write("   - √ù nghƒ©a: T·ªâ l·ªá l·ªói khi agent th·ª±c thi tools\n\n")
        
        f.write("üìà TH·ªêNG K√ä CHI TI·∫æT:\n")
        f.write("-" * 30 + "\n")
        
        # Chi ti·∫øt theo agent
        for agent in results_df['Agent'].unique():
            agent_data = results_df[results_df['Agent'] == agent]
            f.write(f"\nü§ñ {agent}:\n")
            
            for difficulty in ['d·ªÖ', 'kh√≥']:
                diff_data = agent_data[agent_data['Difficulty'] == difficulty]
                if len(diff_data) > 0:
                    row = diff_data.iloc[0]
                    f.write(f"   üìù C√¢u {difficulty}:\n")
                    f.write(f"      - S·ªë m·∫´u: {row['Sample_Count']}\n")
                    f.write(f"      - Accuracy: {row['Accuracy']:.4f} ({row['Accuracy']*100:.2f}%)\n")
                    f.write(f"      - F1 Score: {row['F1_Score']:.4f}\n")
                    f.write(f"      - Precision: {row['Precision']:.4f}\n")
                    f.write(f"      - Recall: {row['Recall']:.4f}\n")
                    f.write(f"      - Tool Fail Rate: {row['Tool_Fail_Rate']:.4f} ({row['Tool_Fail_Rate']*100:.2f}%)\n")
        
        f.write(f"\nüìã GROUND TRUTH INFORMATION:\n")
        f.write("-" * 30 + "\n")
        f.write(f"S·ªë l∆∞·ª£ng ground truth queries: {len(analyzer.ground_truth_tools)}\n")
        f.write("Tools ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ground truth:\n")
        all_tools = set()
        for tools in analyzer.ground_truth_tools.values():
            all_tools.update(tools)
        for tool in sorted(all_tools):
            f.write(f"   - {tool}\n")

def create_full_analysis_report(results_df, base_path="evaluation_analysis/results/detailed_reports"):
    """T·∫°o b√°o c√°o ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"""
    base_path = Path(base_path)
    
    with open(base_path / "full_analysis_report.txt", 'w', encoding='utf-8') as f:
        f.write("üìä B√ÅO C√ÅO PH√ÇN T√çCH ƒê·∫¶Y ƒê·ª¶ - SO S√ÅNH HI·ªÜU SU·∫§T C√ÅC AGENT\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"üìÖ Th·ªùi gian t·∫°o b√°o c√°o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("üìã T·ªîNG QUAN D·ªÆ LI·ªÜU:\n")
        f.write("-" * 40 + "\n")
        total_samples = results_df['Sample_Count'].sum()
        agents_count = len(results_df['Agent'].unique())
        f.write(f"‚Ä¢ T·ªïng s·ªë m·∫´u ƒë√°nh gi√°: {total_samples}\n")
        f.write(f"‚Ä¢ S·ªë agent ƒë∆∞·ª£c so s√°nh: {agents_count}\n")
        f.write(f"‚Ä¢ S·ªë ƒë·ªô kh√≥: {len(results_df['Difficulty'].unique())}\n")
        f.write(f"‚Ä¢ Metrics ƒë√°nh gi√°: Accuracy, F1 Score, Precision, Recall, Tool Fail Rate\n\n")
        
        f.write("üìä B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT:\n")
        f.write("-" * 40 + "\n")
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.4f}'.format)
        f.write(results_df.to_string(index=False))
        f.write("\n\n")
        
        f.write("üìà PH√ÇN T√çCH THEO AGENT:\n")
        f.write("-" * 40 + "\n")
        summary = results_df.groupby('Agent').agg({
            'Accuracy': ['mean', 'std'],
            'F1_Score': ['mean', 'std'],
            'Tool_Fail_Rate': ['mean', 'std']
        }).round(4)
        
        for agent in summary.index:
            f.write(f"\nü§ñ {agent}:\n")
            f.write(f"   ‚Ä¢ Accuracy: {summary.loc[agent, ('Accuracy', 'mean')]:.4f} ¬± {summary.loc[agent, ('Accuracy', 'std')]:.4f}\n")
            f.write(f"   ‚Ä¢ F1 Score: {summary.loc[agent, ('F1_Score', 'mean')]:.4f} ¬± {summary.loc[agent, ('F1_Score', 'std')]:.4f}\n")
            f.write(f"   ‚Ä¢ Tool Fail Rate: {summary.loc[agent, ('Tool_Fail_Rate', 'mean')]:.4f} ¬± {summary.loc[agent, ('Tool_Fail_Rate', 'std')]:.4f}\n")
        
        f.write("\nüìä PH√ÇN T√çCH THEO ƒê·ªò KH√ì:\n")
        f.write("-" * 40 + "\n")
        
        for difficulty in ['d·ªÖ', 'kh√≥']:
            diff_data = results_df[results_df['Difficulty'] == difficulty]
            f.write(f"\nüìù C√¢u {difficulty}:\n")
            f.write(f"   ‚Ä¢ Accuracy trung b√¨nh: {diff_data['Accuracy'].mean():.4f}\n")
            f.write(f"   ‚Ä¢ F1 Score trung b√¨nh: {diff_data['F1_Score'].mean():.4f}\n")
            f.write(f"   ‚Ä¢ Tool Fail Rate trung b√¨nh: {diff_data['Tool_Fail_Rate'].mean():.4f}\n")
            f.write(f"   ‚Ä¢ Agent t·ªët nh·∫•t (Accuracy): {diff_data.loc[diff_data['Accuracy'].idxmax(), 'Agent']}\n")
            f.write(f"   ‚Ä¢ Agent t·ªët nh·∫•t (F1): {diff_data.loc[diff_data['F1_Score'].idxmax(), 'Agent']}\n")

def main():
    parser = argparse.ArgumentParser(description='Ch·∫°y analysis so s√°nh c√°c agent')
    parser.add_argument('--charts-only', action='store_true', help='Ch·ªâ t·∫°o bi·ªÉu ƒë·ªì')
    parser.add_argument('--metrics-only', action='store_true', help='Ch·ªâ t√≠nh metrics')
    args = parser.parse_args()
    
    print("üöÄ B·∫ÆT ƒê·∫¶U ANALYSIS SO S√ÅNH C√ÅC AGENT")
    print("="*50)
    
    # T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
    print("\nüìÅ T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c...")
    create_folder_structure()
    
    # Initialize analyzer
    analyzer = AgentAnalyzer("data_eval/results")
    
    # Load data
    print("\nüì• Loading d·ªØ li·ªáu...")
    if not analyzer.load_agent_data():
        print("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu agent!")
        return
    
    if not analyzer.load_ground_truth("data_eval/synthetic_data/synthetic_news.csv"):
        print("‚ùå Kh√¥ng th·ªÉ load ground truth!")
        return
    
    # Analyze by difficulty
    print("\nüîç Ph√¢n t√≠ch theo ƒë·ªô kh√≥...")
    results_df = analyzer.analyze_by_difficulty()
    failed_cases_df = analyzer.analyze_failed_cases()
    
    if not args.charts_only:
        # Save metrics separately
        print("\nüíæ L∆∞u metrics...")
        save_metrics_separately(results_df)
        
        # Create rankings
        print("\nüèÜ T·∫°o rankings...")
        create_individual_rankings(results_df)
        
        # Save raw data
        print("\nüíø L∆∞u raw data...")
        results_df.to_csv("evaluation_analysis/results/raw_data/complete_results.csv", index=False)
        save_failed_cases_analysis(failed_cases_df)
        
        # Create detailed reports
        print("\nüìÑ T·∫°o b√°o c√°o chi ti·∫øt...")
        save_detailed_reports(results_df, failed_cases_df)
        create_technical_details_report(results_df, analyzer)
        create_full_analysis_report(results_df)
    
    if not args.metrics_only:
        # Create visualizations
        print("\nüìä T·∫°o bi·ªÉu ƒë·ªì...")
        create_visualization_charts(results_df)
    
    print(f"\n‚úÖ HO√ÄN TH√ÄNH! T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c evaluation_analysis/results/")
    print("\nüìã C√°ch xem k·∫øt qu·∫£:")
    print("   ‚Ä¢ Xem ranking nhanh: evaluation_analysis/results/rankings/overall_ranking.txt")
    print("   ‚Ä¢ Xem b√°o c√°o t√≥m t·∫Øt: evaluation_analysis/results/detailed_reports/executive_summary.txt")
    print("   ‚Ä¢ Xem bi·ªÉu ƒë·ªì: evaluation_analysis/results/visualizations/")
    print("   ‚Ä¢ D·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch th√™m: evaluation_analysis/results/raw_data/complete_results.csv")
    
    return results_df

if __name__ == "__main__":
    results = main() 