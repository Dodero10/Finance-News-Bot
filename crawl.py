import json
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException
from bs4 import BeautifulSoup

# C·∫•u h√¨nh Selenium
CHROMEDRIVER_PATH = "E:\Thesis\Crawl\chromedriver-win64\chromedriver.exe"

options = Options()
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")

# Kh√¥ng ch·∫°y ch·∫ø ƒë·ªô headless ƒë·ªÉ xem tr√¨nh duy·ªát
service = Service(CHROMEDRIVER_PATH)
driver = webdriver.Chrome(service=service, options=options)

def click_see_more(max_clicks=200):
    """Nh·∫•n 'Xem th√™m' t·ªëi ƒëa max_clicks l·∫ßn ho·∫∑c ƒë·∫øn khi kh√¥ng c√≤n n√∫t"""
    click_count = 0
    prev_article_count = 0
    
    # L·∫•y s·ªë l∆∞·ª£ng b√†i vi·∫øt ban ƒë·∫ßu
    soup = BeautifulSoup(driver.page_source, "html.parser")
    all_articles = soup.find_all(["h2", "h3"], class_="story__heading")
    prev_article_count = len(all_articles)
    print(f"üìä S·ªë l∆∞·ª£ng b√†i vi·∫øt ban ƒë·∫ßu: {prev_article_count}")
    
    no_change_count = 0  # ƒê·∫øm s·ªë l·∫ßn s·ªë b√†i vi·∫øt kh√¥ng thay ƒë·ªïi
    max_no_change = 3    # S·ªë l·∫ßn t·ªëi ƒëa cho ph√©p kh√¥ng thay ƒë·ªïi

    while click_count < max_clicks:
        try:
            # Cu·ªôn xu·ªëng cu·ªëi trang ƒë·ªÉ ƒë·∫£m b·∫£o n√∫t "Xem th√™m" hi·ªÉn th·ªã
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # T√¨m n√∫t "Xem th√™m"
            try:
                see_more_btn = driver.find_element(By.CLASS_NAME, "control__loadmore")
            except:
                print("‚õî Kh√¥ng t√¨m th·∫•y n√∫t 'Xem th√™m', k·∫øt th√∫c.")
                break
                
            # Ki·ªÉm tra xem n√∫t c√≥ hi·ªÉn th·ªã v√† c√≥ th·ªÉ click ƒë∆∞·ª£c kh√¥ng
            if not see_more_btn.is_displayed() or not see_more_btn.is_enabled():
                print("‚õî N√∫t 'Xem th√™m' kh√¥ng kh·∫£ d·ª•ng, k·∫øt th√∫c.")
                break
                
            driver.execute_script("arguments[0].scrollIntoView();", see_more_btn)
            time.sleep(2)  # ƒê·∫£m b·∫£o th·ªùi gian cho trang t·∫£i ƒë·∫ßy ƒë·ªß
            
            # Click n√∫t b·∫±ng JavaScript ƒë·ªÉ ƒë·∫£m b·∫£o click ƒë∆∞·ª£c
            driver.execute_script("arguments[0].click();", see_more_btn)
            click_count += 1
            print(f"üîÑ ƒê√£ b·∫•m 'Xem th√™m' l·∫ßn {click_count}/{max_clicks} ...")
            
            # ƒê·ª£i ƒë·ªÉ trang t·∫£i th√™m b√†i vi·∫øt
            time.sleep(7)  # TƒÉng th·ªùi gian ch·ªù l√™n ƒë·ªÉ trang c√≥ th·ªùi gian ph·∫£n h·ªìi
            
            # Ki·ªÉm tra s·ªë l∆∞·ª£ng b√†i vi·∫øt hi·ªÉn th·ªã sau khi b·∫•m
            soup = BeautifulSoup(driver.page_source, "html.parser")
            visible_articles = []
            
            # T√¨m t·∫•t c·∫£ c√°c th·∫ª h2 v√† h3 v·ªõi class story__heading
            all_articles = soup.find_all(["h2", "h3"], class_="story__heading")
            current_article_count = len(all_articles)
            print(f"üìä S·ªë l∆∞·ª£ng b√†i vi·∫øt hi·ªán t·∫°i: {current_article_count}")
            
            # Ki·ªÉm tra xem s·ªë b√†i vi·∫øt c√≥ tƒÉng kh√¥ng
            if current_article_count <= prev_article_count:
                no_change_count += 1
                print(f"‚ö†Ô∏è S·ªë l∆∞·ª£ng b√†i vi·∫øt kh√¥ng tƒÉng l·∫ßn {no_change_count}/{max_no_change}")
                if no_change_count >= max_no_change:
                    print("‚õî ƒê√£ ƒë·∫°t s·ªë l·∫ßn kh√¥ng tƒÉng t·ªëi ƒëa, k·∫øt th√∫c.")
                    break
            else:
                no_change_count = 0  # Reset bi·∫øn ƒë·∫øm n·∫øu s·ªë b√†i vi·∫øt tƒÉng
                
            prev_article_count = current_article_count
            
            # Th√™m th·ªùi gian ngh·ªâ ƒë·ªÉ tr√°nh t·∫£i trang qu√° nhanh
            time.sleep(3)

        except Exception as e:
            print(f"‚õî L·ªói khi b·∫•m 'Xem th√™m': {str(e)}")
            break
            
    print(f"‚úÖ ƒê√£ ho√†n th√†nh vi·ªác t·∫£i th√™m b√†i vi·∫øt. T·ªïng s·ªë l·∫ßn b·∫•m: {click_count}/{max_clicks}")

def get_article_links():
    """L·∫•y danh s√°ch link t·ª´ t·∫•t c·∫£ c√°c b√†i vi·∫øt tr√™n trang"""
    url = "https://www.tinnhanhchungkhoan.vn/ck-quoc-te/"
    driver.get(url)
    time.sleep(3)
    click_see_more()
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    
    # T√¨m ch√≠nh x√°c div theo y√™u c·∫ßu
    category_timeline = soup.find("div", class_="category-timeline")
    
    if category_timeline:
        # T√¨m div con v·ªõi class="box-content content-list"
        content_list = category_timeline.find("div", class_="box-content content-list", attrs={"data-source": "zone-timeline-13"})
        
        if content_list:
            print("‚úÖ ƒê√£ t√¨m th·∫•y div target ch√≠nh x√°c")
            # ƒê·∫øm s·ªë l∆∞·ª£ng b√†i vi·∫øt trong div n√†y
            articles_in_target = content_list.find_all("article", class_="story")
            print(f"üìä S·ªë b√†i vi·∫øt trong div target: {len(articles_in_target)}")
            
            # T√¨m t·∫•t c·∫£ c√°c th·∫ª h2 v√† h3 v·ªõi class=story__heading trong div n√†y
            all_headings = content_list.find_all(["h2", "h3"], class_="story__heading")
            print(f"üìä T·ªïng s·ªë heading trong div target: {len(all_headings)}")
            
            links = []
            for heading in all_headings:
                link_tag = heading.find("a", class_="cms-link", href=True)
                if link_tag:
                    full_url = link_tag["href"]
                    # Ki·ªÉm tra xem URL c√≥ ƒë·∫ßy ƒë·ªß domain ch∆∞a
                    if not full_url.startswith("http"):
                        if full_url.startswith("/"):
                            full_url = "https://www.tinnhanhchungkhoan.vn" + full_url
                        else:
                            full_url = "https://www.tinnhanhchungkhoan.vn/" + full_url
                    links.append(full_url)
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y div.box-content.content-list trong div.category-timeline")
            # Fallback: t√¨m ki·∫øm tr√™n to√†n b·ªô trang
            all_headings = soup.find_all(["h2", "h3"], class_="story__heading")
            print(f"üìä T·ªïng s·ªë b√†i vi·∫øt tr√™n to√†n trang: {len(all_headings)}")
            
            links = []
            for heading in all_headings:
                link_tag = heading.find("a", class_="cms-link", href=True)
                if link_tag:
                    full_url = link_tag["href"]
                    # Ki·ªÉm tra xem URL c√≥ ƒë·∫ßy ƒë·ªß domain ch∆∞a
                    if not full_url.startswith("http"):
                        if full_url.startswith("/"):
                            full_url = "https://www.tinnhanhchungkhoan.vn" + full_url
                        else:
                            full_url = "https://www.tinnhanhchungkhoan.vn/" + full_url
                    links.append(full_url)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y div.category-timeline")
        # Fallback: t√¨m ki·∫øm tr√™n to√†n b·ªô trang
        all_headings = soup.find_all(["h2", "h3"], class_="story__heading")
        print(f"üìä T·ªïng s·ªë b√†i vi·∫øt tr√™n to√†n trang: {len(all_headings)}")
        
        links = []
        for heading in all_headings:
            link_tag = heading.find("a", class_="cms-link", href=True)
            if link_tag:
                full_url = link_tag["href"]
                # Ki·ªÉm tra xem URL c√≥ ƒë·∫ßy ƒë·ªß domain ch∆∞a
                if not full_url.startswith("http"):
                    if full_url.startswith("/"):
                        full_url = "https://www.tinnhanhchungkhoan.vn" + full_url
                    else:
                        full_url = "https://www.tinnhanhchungkhoan.vn/" + full_url
                links.append(full_url)

    # Lo·∫°i b·ªè tr√πng l·∫∑p
    links = list(dict.fromkeys(links))
    print(f"‚úÖ ƒê√£ thu th·∫≠p {len(links)} b√†i vi·∫øt kh√¥ng tr√πng l·∫∑p.")
    return links

def get_article_details(url):
    """L·∫•y th√¥ng tin chi ti·∫øt t·ª´ b√†i vi·∫øt"""
    driver.get(url)
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")

    title = soup.find("h1", class_="article__header").text.strip() if soup.find("h1", class_="article__header") else ""
    author = soup.find("a", class_="cms-author").text.strip() if soup.find("a", class_="cms-author") else "Kh√¥ng r√µ"
    time_published = soup.find("time", class_="time").text.strip() if soup.find("time", class_="time") else "Kh√¥ng r√µ"
    summary = soup.find("div", class_="article__sapo").text.strip() if soup.find("div", class_="article__sapo") else ""
    
    # X·ª≠ l√Ω content sang ƒë·ªãnh d·∫°ng markdown
    content_markdown = ""
    content_div = soup.find("div", class_="article__body", attrs={"itemprop": "articleBody"})
    if content_div:
        # Lo·∫°i b·ªè c√°c ph·∫ßn qu·∫£ng c√°o
        for ads in content_div.find_all("div", class_="ads_middle"):
            ads.extract()
            
        # X·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
        for element in content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'table', 'ul', 'ol', 'figure', 'div']):
            if element.name == 'p':
                # Ki·ªÉm tra xem ƒëo·∫°n vƒÉn c√≥ ch·ª©a h√¨nh ·∫£nh kh√¥ng
                img = element.find('img')
                if img:
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', 'H√¨nh ·∫£nh')
                    # Ki·ªÉm tra xem c√≥ ph·∫£i l√† h√¨nh ·∫£nh lazy-load kh√¥ng
                    data_src = img.get('data-src', '')
                    data_original = img.get('data-original', '')
                    
                    # ∆Øu ti√™n l·∫•y URL th·ª±c c·ªßa h√¨nh ·∫£nh thay v√¨ placeholder
                    if data_src and not img_src.startswith('data:'):
                        img_src = data_src
                    elif data_original and not img_src.startswith('data:'):
                        img_src = data_original
                    elif img_src.startswith('data:'):
                        # T√¨m data-src ho·∫∑c data-original n·∫øu src l√† base64
                        if data_src:
                            img_src = data_src
                        elif data_original:
                            img_src = data_original
                        else:
                            # N·∫øu kh√¥ng t√¨m th·∫•y url th·∫≠t, b·ªè qua h√¨nh ·∫£nh n√†y
                            text = element.text.strip()
                            if text:
                                # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng ƒë·∫≠m
                                if element.find('strong'):
                                    for strong in element.find_all('strong'):
                                        strong_text = strong.text
                                        text = text.replace(strong_text, f"**{strong_text}**")
                                content_markdown += f"{text}\n\n"
                            continue
                    
                    # Th√™m h√¨nh ·∫£nh v√†o markdown
                    content_markdown += f"![{img_alt}]({img_src})\n\n"
                else:
                    text = element.text.strip()
                    if not text:  # B·ªè qua ƒëo·∫°n vƒÉn tr·ªëng
                        continue
                        
                    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng ƒë·∫≠m
                    if element.find('strong'):
                        for strong in element.find_all('strong'):
                            strong_text = strong.text
                            text = text.replace(strong_text, f"**{strong_text}**")
                    
                    content_markdown += f"{text}\n\n"
                    
            elif element.name.startswith('h'):
                # X·ª≠ l√Ω ti√™u ƒë·ªÅ
                level = int(element.name[1])
                text = element.text.strip()
                content_markdown += f"{'#' * level} {text}\n\n"
                
            elif element.name == 'table':
                # X·ª≠ l√Ω b·∫£ng c√≥ ch·ª©a h√¨nh ·∫£nh
                img = element.find('img')
                if img:
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', 'H√¨nh ·∫£nh')
                    
                    # Ki·ªÉm tra lazy-loaded images
                    data_src = img.get('data-src', '')
                    data_original = img.get('data-original', '')
                    
                    if data_src and (not img_src or img_src.startswith('data:')):
                        img_src = data_src
                    elif data_original and (not img_src or img_src.startswith('data:')):
                        img_src = data_original
                    
                    if img_src and not img_src.startswith('data:'):
                        content_markdown += f"![{img_alt}]({img_src})\n\n"
                
            elif element.name == 'figure':
                # X·ª≠ l√Ω figure (th∆∞·ªùng ch·ª©a h√¨nh ·∫£nh)
                img = element.find('img')
                if img:
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', 'H√¨nh ·∫£nh')
                    
                    # Ki·ªÉm tra lazy-loaded images
                    data_src = img.get('data-src', '')
                    data_original = img.get('data-original', '')
                    
                    if data_src and (not img_src or img_src.startswith('data:')):
                        img_src = data_src
                    elif data_original and (not img_src or img_src.startswith('data:')):
                        img_src = data_original
                    
                    if img_src and not img_src.startswith('data:'):
                        # T√¨m caption n·∫øu c√≥
                        figcaption = element.find('figcaption')
                        if figcaption:
                            img_alt = figcaption.text.strip()
                        
                        content_markdown += f"![{img_alt}]({img_src})\n\n"
            
            elif element.name == 'div' and element.find('img'):
                # X·ª≠ l√Ω div ch·ª©a h√¨nh ·∫£nh
                for img in element.find_all('img'):
                    img_src = img.get('src', '')
                    img_alt = img.get('alt', 'H√¨nh ·∫£nh')
                    
                    # Ki·ªÉm tra lazy-loaded images
                    data_src = img.get('data-src', '')
                    data_original = img.get('data-original', '')
                    
                    if data_src and (not img_src or img_src.startswith('data:')):
                        img_src = data_src
                    elif data_original and (not img_src or img_src.startswith('data:')):
                        img_src = data_original
                    
                    if img_src and not img_src.startswith('data:'):
                        content_markdown += f"![{img_alt}]({img_src})\n\n"
                
            elif element.name == 'ul':
                for li in element.find_all('li'):
                    content_markdown += f"* {li.text.strip()}\n"
                content_markdown += "\n"
                
            elif element.name == 'ol':
                for i, li in enumerate(element.find_all('li'), 1):
                    content_markdown += f"{i}. {li.text.strip()}\n"
                content_markdown += "\n"
    
    # L·∫•y ƒë√∫ng tag t·ª´ div.article__tag
    tags = []
    article_tag_div = soup.find("div", class_="article__tag")
    if article_tag_div:
        box_content = article_tag_div.find("div", class_="box-content")
        if box_content:
            for tag_link in box_content.find_all("a"):
                tag_text = tag_link.text.strip()
                if tag_text:
                    tags.append(tag_text)
    
    # L·∫•y ·∫£nh ƒë·∫°i di·ªán
    image_tag = soup.find("figure", class_="article__avatar")
    image_url = ""
    if image_tag and image_tag.find("img"):
        image_url = image_tag.find("img")["src"]

    print("Title: ", title)
    print("Author: ", author)
    print("Time published: ", time_published)
    print("Summary: ", summary)
    print("Content: ", content_markdown)
    print("Tags: ", tags)
    print("Image URL: ", image_url)

    return {
        "title": title,
        "author": author,
        "time_published": time_published,
        "summary": summary,
        "content": content_markdown,
        "tags": tags,
        "image_url": image_url,
        "url": url
    }

def save_to_json(data, filename="tinnhanhchungkhoan_articles.json"):
    """L∆∞u d·ªØ li·ªáu v√†o file JSON"""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"üíæ ƒê√£ l∆∞u {len(data)} b√†i vi·∫øt v√†o {filename}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file JSON: {e}")

def append_to_json(article, filename="tinnhanhchungkhoan_articles.json"):
    """Th√™m m·ªôt b√†i vi·∫øt v√†o file JSON hi·ªán c√≥ ho·∫∑c t·∫°o file m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i"""
    try:
        # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng
        try:
            with open(filename, "r", encoding="utf-8") as f:
                try:
                    # ƒê·ªçc d·ªØ li·ªáu hi·ªán t·∫°i
                    articles = json.load(f)
                except json.JSONDecodeError:
                    # N·∫øu file r·ªóng ho·∫∑c kh√¥ng ph·∫£i JSON h·ª£p l·ªá, t·∫°o m·∫£ng m·ªõi
                    articles = []
        except FileNotFoundError:
            # N·∫øu file ch∆∞a t·ªìn t·∫°i, t·∫°o m·∫£ng m·ªõi
            articles = []
        
        # Th√™m b√†i vi·∫øt m·ªõi
        articles.append(article)
        
        # Ghi l·∫°i v√†o file
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=4)
        
        print(f"üíæ ƒê√£ l∆∞u b√†i vi·∫øt m·ªõi v√†o {filename}, t·ªïng s·ªë: {len(articles)}")
    except Exception as e:
        print(f"‚ùå L·ªói khi th√™m b√†i vi·∫øt v√†o file JSON: {e}")

def test_click_and_crawl_2_articles():
    """Test: Nh·∫•n 'Xem th√™m' m·ªôt l·∫ßn v√† crawl 2 b√†i ƒë·∫ßu ti√™n"""
    url = "https://www.tinnhanhchungkhoan.vn/nhan-dinh/"
    driver.get(url)
    print("‚úÖ ƒê√£ m·ªü trang web")
    time.sleep(3)
    
    # Cu·ªôn xu·ªëng cu·ªëi trang ƒë·ªÉ ƒë·∫£m b·∫£o n√∫t "Xem th√™m" hi·ªÉn th·ªã
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    
    try:
        # T√¨m v√† click n√∫t "Xem th√™m"
        see_more_btn = driver.find_element(By.CLASS_NAME, "control__loadmore")
        driver.execute_script("arguments[0].scrollIntoView();", see_more_btn)
        time.sleep(2)
        driver.execute_script("arguments[0].click();", see_more_btn)
        print("‚úÖ ƒê√£ nh·∫•n n√∫t 'Xem th√™m'")
        time.sleep(7)  # ƒê·ª£i trang load th√™m b√†i vi·∫øt
    except Exception as e:
        print(f"‚õî L·ªói khi nh·∫•n 'Xem th√™m': {str(e)}")
    
    # L·∫•y t·∫•t c·∫£ c√°c b√†i vi·∫øt
    soup = BeautifulSoup(driver.page_source, "html.parser")
    
    # T√¨m div ch·ª©a b√†i vi·∫øt
    category_timeline = soup.find("div", class_="category-timeline")
    if category_timeline:
        content_list = category_timeline.find("div", class_="box-content content-list")
        if content_list:
            print("‚úÖ ƒê√£ t√¨m th·∫•y div target")
            articles = content_list.find_all("article", class_="story")
            print(f"üìä T·ªïng s·ªë b√†i vi·∫øt t√¨m th·∫•y: {len(articles)}")
            
            # L·∫•y 2 b√†i ƒë·∫ßu ti√™n
            links = []
            for i, article in enumerate(articles[:2]):
                heading = article.find(["h2", "h3"], class_="story__heading")
                if heading:
                    link_tag = heading.find("a", class_="cms-link", href=True)
                    if link_tag:
                        full_url = link_tag["href"]
                        if not full_url.startswith("http"):
                            if full_url.startswith("/"):
                                full_url = "https://www.tinnhanhchungkhoan.vn" + full_url
                            else:
                                full_url = "https://www.tinnhanhchungkhoan.vn/" + full_url
                        links.append(full_url)
                        print(f"üîó B√†i {i+1}: {full_url}")
            
            # Kh·ªüi t·∫°o file JSON test
            test_file = "test_articles.json"
            try:
                with open(test_file, "w", encoding="utf-8") as f:
                    json.dump([], f)
                print(f"‚úÖ ƒê√£ t·∫°o file {test_file} m·ªõi")
            except Exception as e:
                print(f"‚ùå L·ªói khi t·∫°o file test: {str(e)}")
            
            # Crawl chi ti·∫øt t·ª´ng b√†i v√† l∆∞u ngay
            for i, link in enumerate(links):
                print(f"\nüìù ƒêang l·∫•y chi ti·∫øt b√†i {i+1}: {link}")
                article = get_article_details(link)
                
                # L∆∞u ngay b√†i vi·∫øt v·ª´a crawl
                append_to_json(article, test_file)
                
                # Hi·ªÉn th·ªã th√¥ng tin t√≥m t·∫Øt
                print(f"\n===== B√ÄI {i+1} =====")
                print(f"Ti√™u ƒë·ªÅ: {article['title']}")
                print(f"T√°c gi·∫£: {article['author']}")
                print(f"Th·ªùi gian: {article['time_published']}")
                print(f"Tags: {', '.join(article['tags'])}")
                print("N·ªôi dung (50 k√Ω t·ª± ƒë·∫ßu):", article['content'][:50] + "...")
            
            print("\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ test v√†o test_articles.json")
            
            # ƒê·ªçc l·∫°i file ƒë·ªÉ ki·ªÉm tra
            try:
                with open(test_file, "r", encoding="utf-8") as f:
                    saved_articles = json.load(f)
                print(f"‚úÖ ƒê√£ ki·ªÉm tra file JSON: C√≥ {len(saved_articles)} b√†i vi·∫øt")
                return saved_articles
            except Exception as e:
                print(f"‚ùå L·ªói khi ƒë·ªçc l·∫°i file test: {str(e)}")
                return []
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y div.box-content.content-list")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y div.category-timeline")
    
    return []

def main():
    links = get_article_links()
    time.sleep(10)
    
    output_file = "tinnhanhchungkhoan_quoc_te.json"
    
    # Kh·ªüi t·∫°o file JSON tr·ªëng n·∫øu ch∆∞a t·ªìn t·∫°i
    try:
        with open(output_file, "r") as f:
            pass
    except FileNotFoundError:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump([], f)
        print(f"‚úÖ ƒê√£ t·∫°o file {output_file} m·ªõi")

    for i, url in enumerate(links, start=1):
        try:
            print(f"üìù [{i}/{len(links)}] ƒêang l·∫•y b√†i vi·∫øt: {url}")
            article = get_article_details(url)
            
            # L∆∞u ngay b√†i vi·∫øt v·ª´a crawl v√†o file JSON
            append_to_json(article, output_file)
            
            # T·∫°m d·ª´ng m·ªôt ch√∫t ƒë·ªÉ tr√°nh t·∫£i qu√° nhanh
            time.sleep(1)
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω b√†i vi·∫øt {url}: {e}")
            continue

    print("‚úÖ Ho√†n th√†nh crawl d·ªØ li·ªáu.")
    driver.quit()

if __name__ == "__main__":
    # try:
    #     results = test_click_and_crawl_2_articles()
    #     print("\n‚úÖ Test ho√†n th√†nh!")
    # except Exception as e:
    #     print(f"‚ùå L·ªói: {str(e)}")
    # finally:
    #     time.sleep(5)
    #     driver.quit()
    main()
