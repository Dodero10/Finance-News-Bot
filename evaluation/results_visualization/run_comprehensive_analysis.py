#!/usr/bin/env python3
"""
Runner script for Comprehensive Metrics Analysis

This script runs a complete analysis of all evaluation metrics including:
- Exact Accuracy (Flexible and Strict)
- Partial Accuracy (what you requested - credit for partial tool matches)
- F1 Score, Precision, Recall
- Over-selection and Under-selection rates
- Performance by difficulty level

The Partial Accuracy metric addresses your specific need:
- If expected: ['listing_symbol', 'search_web']
- If agent uses: ['listing_symbol'] â†’ Partial Accuracy = 50% (1/2 tools correct)
- If agent uses: ['listing_symbol', 'search_web', 'retrival_db'] â†’ Partial Accuracy = 100% (2/2 required tools found)

Usage:
    python run_comprehensive_analysis.py
"""

import sys
from pathlib import Path
import pandas as pd

# Add the current directory to Python path
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

from comprehensive_metrics_analyzer import ComprehensiveMetricsAnalyzer

def main():
    """Main function to run the comprehensive analysis"""
    print("ğŸ¯" * 40)
    print("FINANCE NEWS BOT - COMPREHENSIVE METRICS ANALYSIS")
    print("ğŸ¯" * 40)
    print()
    
    print("ğŸ“‹ This analysis provides a complete view of all evaluation metrics:")
    print("   â€¢ Exact Accuracy (Flexible): Perfect tool set match, order doesn't matter")
    print("   â€¢ Exact Accuracy (Strict): Perfect tool set match, order matters")
    print("   â€¢ Partial Accuracy: % of required tools correctly identified")
    print("   â€¢ F1 Score: Harmonic mean of precision and recall")
    print("   â€¢ Precision: % of selected tools that were correct")
    print("   â€¢ Recall: % of required tools that were found")
    print("   â€¢ Over-selection Rate: % of selected tools that were unnecessary")
    print("   â€¢ Under-selection Rate: % of required tools that were missed")
    print()
    
    print("ğŸ’¡ Key Features:")
    print("   âœ¨ Partial Accuracy gives credit for partial tool selection")
    print("   ğŸ“Š Comprehensive comparison across all metrics")
    print("   ğŸšï¸ Analysis by query difficulty level")
    print("   ğŸ” Tool selection behavior analysis")
    print()
    
    try:
        # Initialize the analyzer
        print("ğŸš€ Initializing comprehensive metrics analyzer...")
        analyzer = ComprehensiveMetricsAnalyzer()
        
        # Run complete analysis
        results, summary_df, difficulty_df = analyzer.run_complete_comprehensive_analysis()
        
        print("\n" + "ğŸ“Š" * 25)
        print("COMPREHENSIVE METRICS INSIGHTS")
        print("ğŸ“Š" * 25)
        
        # Parse numerical values for ranking
        summary_numeric = summary_df.copy()
        numeric_cols = ['Flexible Accuracy', 'Strict Accuracy', 'Partial Accuracy', 'F1 Score', 'Precision', 'Recall']
        
        for col in numeric_cols:
            try:
                summary_numeric[col] = pd.to_numeric(summary_numeric[col].str.strip(), errors='coerce')
            except Exception as e:
                print(f"Warning: Could not convert column {col} to numeric: {e}")
                summary_numeric[col] = 0.0
        
        print("\nğŸ† AGENT PERFORMANCE RANKING:")
        print("-" * 60)
        
        # Rank by different metrics
        rankings = {}
        for metric in ['Flexible Accuracy', 'Strict Accuracy', 'Partial Accuracy', 'F1 Score']:
            sorted_df = summary_numeric.sort_values(metric, ascending=False)
            rankings[metric] = list(sorted_df['Agent'])
        
        print("ğŸ“ˆ Rankings by different metrics:")
        for metric, ranked_agents in rankings.items():
            print(f"   {metric:>18}: {' > '.join(ranked_agents)}")
        
        # Overall performance summary
        print(f"\nğŸ¯ DETAILED PERFORMANCE BREAKDOWN:")
        print("-" * 60)
        
        for _, row in summary_numeric.iterrows():
            agent = row['Agent']
            try:
                over_selection = float(summary_numeric[summary_numeric['Agent']==agent]['Over-selection Rate'].iloc[0])
                print(f"\nğŸ¤– {agent}:")
                print(f"   â€¢ Flexible Accuracy: {float(row['Flexible Accuracy']):.3f} (perfect match, any order)")
                print(f"   â€¢ Strict Accuracy:   {float(row['Strict Accuracy']):.3f} (perfect match, exact order)")
                print(f"   â€¢ Partial Accuracy:  {float(row['Partial Accuracy']):.3f} â­ (credit for partial matches)")
                print(f"   â€¢ F1 Score:          {float(row['F1 Score']):.3f}")
                print(f"   â€¢ Precision:         {float(row['Precision']):.3f} (quality of selections)")
                print(f"   â€¢ Over-selection:    {over_selection:.3f} (unnecessary tools)")
            except Exception as e:
                print(f"   âš ï¸ Error formatting metrics for {agent}: {str(e)}")
        
        # Key insights about partial accuracy
        best_partial = summary_numeric.loc[summary_numeric['Partial Accuracy'].idxmax()]
        worst_partial = summary_numeric.loc[summary_numeric['Partial Accuracy'].idxmin()]
        
        print(f"\nâœ¨ PARTIAL ACCURACY INSIGHTS:")
        print("-" * 50)
        print(f"ğŸ¥‡ Best: {best_partial['Agent']} with {best_partial['Partial Accuracy']:.3f} partial accuracy")
        print(f"ğŸ¥‰ Worst: {worst_partial['Agent']} with {worst_partial['Partial Accuracy']:.3f} partial accuracy")
        
        # Calculate improvement from strict to partial
        improvements = summary_numeric['Partial Accuracy'] - summary_numeric['Strict Accuracy']
        avg_improvement = improvements.mean()
        print(f"ğŸ“ˆ Average improvement from Strict to Partial Accuracy: {avg_improvement:.3f}")
        
        if avg_improvement > 0.1:
            print("   ğŸ’¡ Significant improvement! Agents do select some correct tools even when not perfect.")
        
        # Analysis of selection behavior
        print(f"\nğŸ¯ TOOL SELECTION BEHAVIOR:")
        print("-" * 50)
        
        avg_over_selection = summary_numeric['Over-selection Rate'].mean()
        avg_under_selection = summary_numeric['Under-selection Rate'].mean()
        
        print(f"   â€¢ Average Over-selection Rate: {avg_over_selection:.3f}")
        print(f"   â€¢ Average Under-selection Rate: {avg_under_selection:.3f}")
        
        if avg_over_selection > avg_under_selection:
            print("   ğŸ“Š Agents tend to select TOO MANY tools (over-selection)")
            print("   ğŸ’¡ Focus training on being more selective")
        else:
            print("   ğŸ“Š Agents tend to MISS required tools (under-selection)")
            print("   ğŸ’¡ Focus training on tool coverage completeness")
        
        # Difficulty analysis
        if not difficulty_df.empty:
            print(f"\nğŸ“š DIFFICULTY IMPACT ANALYSIS:")
            print("-" * 50)
            
            for _, row in difficulty_df.iterrows():
                agent = row['Agent']
                easy_partial = row['Easy_Partial']
                hard_partial = row['Hard_Partial']
                gap = easy_partial - hard_partial
                
                print(f"   {agent:>12}: Easy {easy_partial:.3f} | Hard {hard_partial:.3f} | Gap {gap:.3f}")
            
            avg_gap = (difficulty_df['Easy_Partial'] - difficulty_df['Hard_Partial']).mean()
            print(f"\n   Average Difficulty Gap (Partial Accuracy): {avg_gap:.3f}")
            
            if avg_gap > 0.15:
                print("   âš ï¸  Large difficulty gap - agents struggle with complex queries")
            elif avg_gap > 0.05:
                print("   âš–ï¸  Moderate difficulty gap - expected performance difference")
            else:
                print("   âœ… Small difficulty gap - agents handle complexity well")
        
        print(f"\nğŸ“ GENERATED ANALYSIS FILES:")
        print("-" * 50)
        
        # Define output directory structure
        output_dir = Path("evaluation/results_visualization/figures")
        figures_structure = {
            'comprehensive': [
                "comprehensive_metrics_comparison.png",
                "metrics_heatmap.png",
                "comprehensive_metrics_summary.csv"
            ],
            'accuracy': [
                "accuracy_types_comparison.png"
            ],
            'selection': [
                "selection_behavior_analysis.png"
            ],
            'difficulty': [
                "comprehensive_difficulty_analysis.png"
            ]
        }
        
        # Create directories if they don't exist
        for subdir in figures_structure.keys():
            (output_dir / subdir).mkdir(parents=True, exist_ok=True)
        
        # Check and report file status by category
        for category, files in figures_structure.items():
            print(f"\n{category.upper()} Metrics:")
            for file in files:
                file_path = output_dir / category / file
                if file_path.exists():
                    print(f"   âœ“ {file}")
                else:
                    print(f"   âœ— {file} (not found)")
        
        print(f"\nğŸ“‚ All comprehensive analysis files saved to: {output_dir}")
        
        print("\n" + "ğŸ’¡" * 25)
        print("ACTIONABLE RECOMMENDATIONS")
        print("ğŸ’¡" * 25)
        
        print("\nğŸ¯ Based on comprehensive analysis:")
        
        # Recommendations based on results
        avg_partial = summary_numeric['Partial Accuracy'].mean()
        avg_precision = summary_numeric['Precision'].mean()
        avg_over_selection = summary_numeric['Over-selection Rate'].mean()
        
        print(f"\n1. ğŸ“ˆ Use Partial Accuracy for Better Insights:")
        print(f"   â€¢ Current average: {avg_partial:.3f} (vs strict accuracy which is much lower)")
        print("   â€¢ This metric better reflects agent capabilities")
        print("   â€¢ Focus training on improving partial accuracy first")
        
        if avg_partial < 0.7:
            print(f"\n2. ğŸ”§ Improve Tool Coverage (Partial Accuracy < 70%):")
            print("   â€¢ Agents miss too many required tools")
            print("   â€¢ Add more diverse tool selection examples")
            print("   â€¢ Implement tool selection confidence scoring")
        
        if avg_precision < 0.7:
            print(f"\n3. ğŸ¯ Improve Selection Precision (< 70%):")
            print("   â€¢ Agents select too many incorrect tools")
            print("   â€¢ Add negative examples showing wrong tool choices")
            print("   â€¢ Implement tool relevance filtering")
        
        if avg_over_selection > 0.3:
            print(f"\n4. âš¡ Reduce Over-selection (> 30%):")
            print("   â€¢ Agents are not selective enough")
            print("   â€¢ Train with examples of minimal tool sets")
            print("   â€¢ Implement tool necessity checks")
        
        print(f"\n5. ğŸ“Š Recommended Evaluation Approach:")
        print("   â€¢ Primary metric: Partial Accuracy (most informative)")
        print("   â€¢ Secondary metrics: F1 Score and Precision")
        print("   â€¢ Monitor: Over-selection and Under-selection rates")
        print("   â€¢ Use Strict Accuracy only for final validation")
        
        print("\n" + "âœ…" * 25)
        print("COMPREHENSIVE ANALYSIS COMPLETED!")
        print("âœ…" * 25)
        
        print(f"\nğŸ“ Your specific question answered:")
        print("   If dataset expects ['listing_symbol', 'search_web']")
        print("   âœ… Agent uses ['listing_symbol', 'search_web'] â†’ 100% partial accuracy")
        print("   âœ… Agent uses ['listing_symbol'] â†’ 50% partial accuracy")
        print("   âŒ Agent uses ['retrival_db', 'search_web'] â†’ 50% partial accuracy")
        print("   ğŸ“Š This metric gives appropriate credit for partial correctness!")
        
    except Exception as e:
        print(f"\nâŒ Error during analysis: {str(e)}")
        print("\nPlease check that all required data files are present:")
        print("- evaluation/data_eval/results/*.csv")
        print("- evaluation/data_eval/synthetic_data/synthetic_news.csv")
        sys.exit(1)

if __name__ == "__main__":
    main() 